import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# --- æ•¸æ“šåŠ è¼‰èˆ‡é è™•ç† (ä½¿ç”¨ Streamlit ç·©å­˜) ---
@st.cache_data
def load_and_preprocess_data():
    iris = load_iris()
    X = pd.DataFrame(iris.data, columns=iris.feature_names)
    y = iris.target

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled_df = pd.DataFrame(X_scaled, columns=iris.feature_names) # è½‰å› DataFrame ä»¥ä¿ç•™åˆ—å

    # åˆ‡åˆ†è¨“ç·´é›†å’Œæ¸¬è©¦é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled_df, y, test_size=0.2, random_state=42, stratify=y
    )
    return X_train, X_test, y_train, y_test, iris.target_names, X.columns.tolist(), scaler # å°‡ feature_names è½‰ç‚ºåˆ—è¡¨

# åœ¨æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚åŠ è¼‰å’Œé è™•ç†æ•¸æ“š
X_train_full, X_test_full, y_train, y_test, target_names, all_feature_names, loaded_scaler = load_and_preprocess_data()

st.title('MLP æ¨¡å‹è¨“ç·´èˆ‡è¶…åƒæ•¸äº’å‹•å¼æ¢ç´¢')
st.markdown('é€éèª¿æ•´å·¦å´é‚Šæ¬„çš„åƒæ•¸ä¾†è¨“ç·´ MLP æ¨¡å‹ä¸¦è§€å¯Ÿçµæœã€‚')

# --- å´é‚Šæ¬„åƒæ•¸è¨­å®š ---
st.sidebar.header('MLP æ¨¡å‹è¶…åƒæ•¸è¨­å®š')

# æ–°å¢çš„ç‰¹å¾µé¸æ“‡åŠŸèƒ½
st.sidebar.subheader('0. ç‰¹å¾µé¸æ“‡')
selected_features = st.sidebar.multiselect(
    'é¸æ“‡è¦åŒ…å«çš„ç‰¹å¾µ',
    options=all_feature_names,
    default=all_feature_names # é è¨­å…¨é¸
)

if not selected_features:
    st.sidebar.warning("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹ç‰¹å¾µï¼")
    st.stop() # å¦‚æœæ²’æœ‰é¸æ“‡ä»»ä½•ç‰¹å¾µï¼Œå‰‡åœæ­¢æ‡‰ç”¨ç¨‹å¼é‹è¡Œ

# æ ¹æ“šé¸æ“‡çš„ç‰¹å¾µç¯©é¸æ•¸æ“šé›†
X_train = X_train_full[selected_features]
X_test = X_test_full[selected_features]

# 1. æ¨¡å‹è¤‡é›œåº¦ (Hidden Layer Sizes)
st.sidebar.subheader('1. æ¨¡å‹è¤‡é›œåº¦')
hidden_layer_1 = st.sidebar.slider('ç¬¬ä¸€éš±è—å±¤ç¥ç¶“å…ƒæ•¸é‡', 10, 200, 100, step=10)
num_hidden_layers = st.sidebar.radio('éš±è—å±¤æ•¸é‡', [1, 2], index=0)

hidden_layer_sizes = (hidden_layer_1,)
if num_hidden_layers == 2:
    hidden_layer_2 = st.sidebar.slider('ç¬¬äºŒéš±è—å±¤ç¥ç¶“å…ƒæ•¸é‡', 10, 100, 50, step=10)
    hidden_layer_sizes = (hidden_layer_1, hidden_layer_2)

st.sidebar.write(f'è¨­å®šçš„éš±è—å±¤å¤§å°: {hidden_layer_sizes}')

# 2. æ´»åŒ–å‡½æ•¸ (Activation Function)
activation_function = st.sidebar.selectbox(
    '2. æ´»åŒ–å‡½æ•¸ (Activation Function)',
    ['relu', 'logistic', 'tanh', 'identity'],
    index=0 # é è¨­ ReLU
)

# 3. å„ªåŒ–å™¨ / æ±‚è§£å™¨ (Solver)
solver = st.sidebar.selectbox(
    '3. å„ªåŒ–å™¨ / æ±‚è§£å™¨ (Solver)',
    ['adam', 'sgd', 'lbfgs'],
    index=0 # é è¨­ Adam
)

# 4. å­¸ç¿’ç‡ (Learning Rate) - åƒ…é©ç”¨æ–¼ Adam å’Œ SGD
learning_rate_init = 0.001 # é è¨­åˆå§‹å­¸ç¿’ç‡
if solver in ['adam', 'sgd']:
    st.sidebar.subheader('4. å­¸ç¿’ç‡')
    learning_rate = st.sidebar.selectbox(
        'å­¸ç¿’ç‡ç­–ç•¥',
        ['constant', 'invscaling', 'adaptive'],
        index=0 # é è¨­ constant
    )
    learning_rate_init = st.sidebar.number_input('åˆå§‹å­¸ç¿’ç‡', min_value=0.0001, max_value=0.1, value=0.001, step=0.0001, format="%.4f")
else:
    st.sidebar.info('L-BFGS å„ªåŒ–å™¨ä¸ä½¿ç”¨å­¸ç¿’ç‡ç›¸é—œåƒæ•¸ã€‚')
    learning_rate = 'constant' # å° L-BFGS ç„¡æ•ˆï¼Œä½†ç‚º API åƒæ•¸å®Œæ•´æ€§ä¿ç•™

# 5. æ‰¹æ¬¡å¤§å° (Batch Size) - åƒ…é©ç”¨æ–¼ Adam å’Œ SGD
batch_size = 'auto' # é è¨­å€¼
if solver in ['adam', 'sgd']:
    st.sidebar.subheader('5. æ‰¹æ¬¡å¤§å°')
    batch_size_option = st.sidebar.radio('æ‰¹æ¬¡å¤§å°è¨­å®š', ['auto', 'æ‰‹å‹•è¼¸å…¥'], index=0)
    if batch_size_option == 'æ‰‹å‹•è¼¸å…¥':
        batch_size = st.sidebar.number_input('Batch Size', min_value=1, max_value=len(X_train), value=32, step=1)
    else:
        st.sidebar.info('`auto` æœƒä½¿ç”¨ `min(200, n_samples)`ã€‚')
else:
    st.sidebar.info('L-BFGS å„ªåŒ–å™¨ä¸ä½¿ç”¨æ‰¹æ¬¡å¤§å°ã€‚')
    batch_size = 'auto' # å° L-BFGS ç„¡æ•ˆï¼Œä½†ç‚º API åƒæ•¸å®Œæ•´æ€§ä¿ç•™

# 6. è¨“ç·´æ¬¡æ•¸ / Epochs (Max Iter)
st.sidebar.subheader('6. è¨“ç·´æ¬¡æ•¸')
max_iter = st.sidebar.slider('æœ€å¤§è¿­ä»£æ¬¡æ•¸ (Max Iter)', 50, 1000, 200, step=50)

# 7. Early Stopping (ææ—©åœæ­¢)
st.sidebar.subheader('7. ææ—©åœæ­¢ (Early Stopping)')
early_stopping = st.sidebar.checkbox('å•Ÿç”¨ Early Stopping', value=False)
if early_stopping:
    validation_fraction = st.sidebar.slider('é©—è­‰é›†æ¯”ä¾‹ (Validation Fraction)', 0.05, 0.5, 0.1, step=0.05)
    n_iter_no_change = st.sidebar.slider('ç„¡æ”¹å–„å®¹å¿è¿­ä»£æ¬¡æ•¸', 10, 100, 10, step=5)
    tol = st.sidebar.number_input('å®¹å¿åº¦ (Tolerance)', min_value=1e-5, max_value=1e-2, value=1e-4, format="%.5f")
else:
    st.sidebar.info('è‹¥ä¸å•Ÿç”¨ Early Stoppingï¼Œæ¨¡å‹æœƒè·‘å®Œæ‰€æœ‰ Max Iterã€‚')
    n_iter_no_change = max_iter + 1
    tol = 1e-4

# 8. Alpha (L2 æ­£è¦åŒ–åƒæ•¸)
st.sidebar.subheader('8. æ­£è¦åŒ–åƒæ•¸')
alpha = st.sidebar.number_input('Alpha (L2 æ­£å‰‡åŒ–å¼·åº¦)', min_value=0.0001, max_value=1.0, value=0.0001, step=0.0001, format="%.4f")


# --- æ¨¡å‹è¨“ç·´èˆ‡è©•ä¼° ---
st.header('æ¨¡å‹è¨“ç·´èˆ‡çµæœ')

# é¡¯ç¤ºç•¶å‰æ¨¡å‹åƒæ•¸è¨­å®š
st.write("ç•¶å‰æ¨¡å‹åƒæ•¸è¨­å®šï¼š")
current_params = {
    "é¸æ“‡çš„ç‰¹å¾µ": selected_features,
    "hidden_layer_sizes": hidden_layer_sizes,
    "activation": activation_function,
    "solver": solver,
    "alpha": alpha,
    "batch_size": batch_size,
    "learning_rate": learning_rate,
    "learning_rate_init": learning_rate_init,
    "max_iter": max_iter,
    "early_stopping": early_stopping,
    "validation_fraction": validation_fraction if early_stopping else 'N/A (Early Stopping Disabled)',
    "n_iter_no_change": n_iter_no_change,
    "tol": tol
}
st.json(current_params)

# åŸ·è¡Œè¨“ç·´æŒ‰éˆ•
if st.button('ğŸš€ è¨“ç·´æ¨¡å‹'):
    if not selected_features:
        st.error("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹ç‰¹å¾µæ‰èƒ½è¨“ç·´æ¨¡å‹ï¼")
    else:
        try:
            # å»ºç«‹ MLP æ¨¡å‹å¯¦ä¾‹
            mlp = MLPClassifier(
                hidden_layer_sizes=hidden_layer_sizes,
                activation=activation_function,
                solver=solver,
                alpha=alpha,
                batch_size=batch_size,
                learning_rate=learning_rate,
                learning_rate_init=learning_rate_init,
                max_iter=max_iter,
                early_stopping=early_stopping,
                validation_fraction=validation_fraction if early_stopping else 0.1, # é€™è£¡è¨­ç‚º 0.1 ä»¥ç¬¦åˆé è¨­å€¼
                n_iter_no_change=n_iter_no_change,
                tol=tol,
                random_state=42
            )
            # è¨“ç·´æ¨¡å‹ï¼Œé¡¯ç¤ºé€²åº¦
            with st.spinner('æ¨¡å‹è¨“ç·´ä¸­ï¼Œè«‹ç¨å€™...'):
                mlp.fit(X_train, y_train)
            st.success('âœ… æ¨¡å‹è¨“ç·´å®Œæˆï¼')

            # é æ¸¬
            y_pred_train = mlp.predict(X_train)
            y_pred_test = mlp.predict(X_test)

            # --- çµæœå±•ç¤º ---
            st.subheader('æ¨¡å‹è©•ä¼°çµæœ')

            col1, col2 = st.columns(2)
            with col1:
                st.metric("è¨“ç·´é›†æº–ç¢ºç‡", f"{accuracy_score(y_train, y_pred_train):.4f}")
            with col2:
                st.metric("æ¸¬è©¦é›†æº–ç¢ºç‡", f"{accuracy_score(y_test, y_pred_test):.4f}")

            st.subheader('åˆ†é¡å ±å‘Š (æ¸¬è©¦é›†)')
            report = classification_report(y_test, y_pred_test, target_names=target_names, output_dict=True)
            st.json(report)

            st.subheader('æ··æ·†çŸ©é™£ (æ¸¬è©¦é›†)')
            cm = confusion_matrix(y_test, y_pred_test)
            fig_cm, ax_cm = plt.subplots()
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                        xticklabels=target_names, yticklabels=target_names, ax=ax_cm)
            ax_cm.set_xlabel('é æ¸¬æ¨™ç±¤')
            ax_cm.set_ylabel('çœŸå¯¦æ¨™ç±¤')
            ax_cm.set_title('æ··æ·†çŸ©é™£')
            st.pyplot(fig_cm)
            plt.close(fig_cm) # é—œé–‰åœ–å½¢ä»¥é‡‹æ”¾å…§å­˜
            fig, ax = plt.subplots()
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                        xticklabels=target_names, yticklabels=target_names, ax=ax)
            ax.set_xlabel('é æ¸¬æ¨™ç±¤', fontproperties='SimHei')  # è¨­å®šè»¸æ¨™ç±¤å­—å‹
            ax.set_ylabel('çœŸå¯¦æ¨™ç±¤', fontproperties='SimHei')
            ax.set_title('æ··æ·†çŸ©é™£', fontproperties='SimHei')  # è¨­å®šæ¨™é¡Œå­—å‹
            plt.setp(ax.get_xticklabels(), fontproperties='SimHei') # è¨­å®šåˆ»åº¦æ¨™ç±¤å­—å‹
            plt.setp(ax.get_yticklabels(), fontproperties='SimHei')
            st.pyplot(fig)
            st.subheader('æå¤±æ›²ç·š')
            if hasattr(mlp, 'loss_curve_') and mlp.loss_curve_ is not None and len(mlp.loss_curve_) > 0:
                fig_loss, ax_loss = plt.subplots()
                ax_loss.plot(mlp.loss_curve_)
                ax_loss.set_xlabel('è¿­ä»£æ¬¡æ•¸')
                ax_loss.set_ylabel('æå¤±')
                ax_loss.set_title('è¨“ç·´æå¤±æ›²ç·š')
                st.pyplot(fig_loss)
                plt.close(fig_loss) # é—œé–‰åœ–å½¢ä»¥é‡‹æ”¾å…§å­˜
            else:
                st.info('ç›®å‰é¸æ“‡çš„å„ªåŒ–å™¨ (L-BFGS) ä¸æœƒç”¢ç”Ÿæå¤±æ›²ç·šã€‚')

            # --- äº’å‹•å¼é æ¸¬ ---
            st.subheader('ğŸ’¡ äº’å‹•å¼é æ¸¬')
            st.write(f'è¼¸å…¥æ‚¨é¸æ“‡çš„ {len(selected_features)} å€‹ç‰¹å¾µå€¼ä¾†é æ¸¬ Iris èŠ±çš„ç¨®é¡ï¼š')

            input_data = {}
            for feature in selected_features: # æ ¹æ“šé¸æ“‡çš„ç‰¹å¾µé¡¯ç¤ºè¼¸å…¥æ¡†
                # è¨ˆç®—åŸå§‹æ•¸æ“šçš„åƒè€ƒç¯„åœ
                feature_idx = all_feature_names.index(feature)
                min_val = X_train_full[feature].min() * loaded_scaler.scale_[feature_idx] + loaded_scaler.mean_[feature_idx]
                max_val = X_train_full[feature].max() * loaded_scaler.scale_[feature_idx] + loaded_scaler.mean_[feature_idx]
                avg_val = X_train_full[feature].mean() * loaded_scaler.scale_[feature_idx] + loaded_scaler.mean_[feature_idx]

                input_data[feature] = st.number_input(
                    f'è¼¸å…¥ {feature} (åƒè€ƒåŸå§‹ç¯„åœ: {min_val:.2f} ~ {max_val:.2f})',
                    value=float(avg_val),
                    step=0.1
                )

            if st.button('é æ¸¬æ–°è³‡æ–™'):
                # å°‡ç”¨æˆ¶è¼¸å…¥è½‰æ›ç‚º DataFrameï¼ŒåªåŒ…å«é¸æ“‡çš„ç‰¹å¾µ
                input_df = pd.DataFrame([input_data], columns=selected_features)
                # ç¢ºä¿ç”¨æˆ¶è¼¸å…¥çš„ç‰¹å¾µé †åºèˆ‡è¨“ç·´è³‡æ–™ä¸€è‡´ï¼Œä¸¦é€²è¡Œæ¨™æº–åŒ–
                # æ³¨æ„ï¼šé€™è£¡çš„ StandardScaler å¿…é ˆæ˜¯è¨“ç·´æ™‚ç”¨çš„é‚£å€‹ï¼Œä¸¦ä¸”è¦è™•ç†æœªé¸æ“‡ç‰¹å¾µçš„æƒ…æ³
                # ç‚ºäº†ç°¡åŒ–ï¼Œé€™è£¡å‡è¨­ loaded_scaler æ˜¯åœ¨æ‰€æœ‰åŸå§‹ç‰¹å¾µä¸Šè¨“ç·´çš„
                # æˆ‘å€‘éœ€è¦å‰µå»ºä¸€å€‹èˆ‡åŸå§‹ X çµæ§‹ç›¸åŒçš„ DataFrameï¼Œæœªé¸æ“‡çš„ç‰¹å¾µè¨­ç‚ºå¹³å‡å€¼
                full_input_df = pd.DataFrame(columns=all_feature_names)
                for f in all_feature_names:
                    if f in input_df.columns:
                        full_input_df[f] = input_df[f]
                    else:
                        # å°æ–¼æœªé¸æ“‡çš„ç‰¹å¾µï¼Œå¡«å…¥å…¶åŸå§‹æ•¸æ“šçš„å¹³å‡å€¼ (æˆ– 0ï¼Œå–æ±ºæ–¼ StandardScaler çš„è¡Œç‚º)
                        # æ›´ç©©å¥çš„åšæ³•æ˜¯ä¿å­˜è¨“ç·´æ™‚çš„ç‰¹å¾µå¹³å‡å€¼
                        # é€™è£¡ç‚ºç°¡åŒ–ï¼Œç›´æ¥å¾åŸå§‹æ•¸æ“šä¸­ç²å–å¹³å‡å€¼
                        original_mean = loaded_scaler.mean_[all_feature_names.index(f)]
                        full_input_df[f] = original_mean

                input_scaled = loaded_scaler.transform(full_input_df)

                prediction_proba = mlp.predict_proba(input_scaled)
                prediction_class = np.argmax(prediction_proba)

                st.write(f"é æ¸¬çš„èŠ±çš„ç¨®é¡æ˜¯: **{target_names[prediction_class]}**")
                st.write("å„é¡åˆ¥æ©Ÿç‡ï¼š")
                proba_df = pd.DataFrame({
                    'ç¨®é¡': target_names,
                    'æ©Ÿç‡': prediction_proba[0]
                })
                st.dataframe(proba_df.set_index('ç¨®é¡'))

        except Exception as e:
            st.error(f"æ¨¡å‹è¨“ç·´æˆ–é æ¸¬éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š\n\n`{e}`\n\nè«‹æª¢æŸ¥åƒæ•¸è¨­å®šï¼Œæˆ–å˜—è©¦èª¿æ•´è¼¸å…¥å€¼ã€‚")

st.markdown("---")
st.markdown("**è³‡æ–™ä¾†æº**ï¼š[Scikit-learn Iris Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)")
st.markdown("**æ‡‰ç”¨ç¨‹å¼éƒ¨ç½²**ï¼š[Streamlit Cloud](https://streamlit.io/cloud)")
st.markdown("æœ¬æ‡‰ç”¨ç”± Streamlit æ§‹å»ºï¼Œç”¨æ–¼æ¼”ç¤º MLP æ¨¡å‹åƒæ•¸èª¿æ•´èˆ‡çµæœè¦–è¦ºåŒ–ã€‚")