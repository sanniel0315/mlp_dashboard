import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support, roc_curve, auc
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score, learning_curve, validation_curve
from sklearn.preprocessing import StandardScaler, label_binarize
from sklearn.multiclass import OneVsRestClassifier
import os 
from matplotlib.font_manager import FontProperties, findfont, findSystemFonts, fontManager 
import joblib
from io import BytesIO
import zipfile
import pytz
from datetime import datetime

# --- é é¢é…ç½® ---
st.set_page_config(
    page_title="MLP æ¨¡å‹è¨“ç·´å™¨",
    page_icon="ğŸ§¬",  
    layout="wide",
    initial_sidebar_state="expanded"
)

# å…ˆå®šç¾©å°ç£æ™‚å€ï¼Œå†ä½¿ç”¨å®ƒ
taiwan_tz = pytz.timezone('Asia/Taipei')
current_time = datetime.now(taiwan_tz)
date_str = current_time.strftime("%Yå¹´%mæœˆ%dæ—¥")
time_str = current_time.strftime("%H:%M:%S")

# --- è‡ªå®šç¾© CSS æ¨£å¼ ---
st.markdown("""
<style>
    /* å…¨å±€æ¨£å¼ */
    .main .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
    }
    
    /* æ¨™ç±¤é æ¨£å¼ */
    button[data-baseweb="tab"] {
        font-size: 24px !important;  /* åŸä¾†æ˜¯18pxï¼Œå¢å¤§è‡³24px */
        font-weight: 800 !important; /* åŠ ç²—ä¸€é» */
        padding: 20px 40px !important; /* åŸä¾†æ˜¯12px 24pxï¼Œå¢å¤§å…§é‚Šè· */
        order-radius: 10px 10px 0 0 !important; /* æ›´å¤§çš„åœ“è§’ */
        margin-right: 10px !important; /* æ›´å¤§çš„æ¨™ç±¤é–“è· */
        border: 2px solid #e0e0e0 !important; /* æ›´ç²—çš„é‚Šæ¡† */
        border-bottom: none !important;
        min-width: 220px !important; /* ç¢ºä¿æ¨™ç±¤æ›´å¯¬ */
        height: auto !important; /* è‡ªå‹•é«˜åº¦ */
        min-height: 70px !important; /* ç¢ºä¿æœ€å°é«˜åº¦ */
        line-height: 1.4 !important; /* èª¿æ•´è¡Œé«˜ */
        transform: scale(1.05); /* ç¨å¾®æ”¾å¤§æ•´å€‹æŒ‰éˆ• */
        box-shadow: 0 -2px 5px rgba(0,0,0,0.05); /* æ·»åŠ è¼•å¾®é™°å½±å¢å¼·ç«‹é«”æ„Ÿ */
    }
    button[data-baseweb="tab"]:hover {
        background-color: #f0f8ff !important; /* æ›´æ˜é¡¯çš„æ‡¸åœæ•ˆæœ */
        transform: scale(1.1) translateY(-2px) !important; /* æ‡¸åœæ™‚æ›´æ˜é¡¯çš„æ”¾å¤§æ•ˆæœ */
        transition: all 0.3s ease !important;
    }
    
    button[data-baseweb="tab"][aria-selected="true"] {
        background-color: #e6f3ff !important; /* é¸ä¸­æ¨™ç±¤çš„èƒŒæ™¯è‰² */
        border-bottom: 3px solid #4dabf7 !important; /* é¸ä¸­æ¨™ç±¤çš„åº•éƒ¨é‚Šæ¡† */
        color: #1a73e8 !important; /* é¸ä¸­æ¨™ç±¤çš„æ–‡å­—é¡è‰² */
    }
    
    div[role="tablist"] {
        border-bottom: 4px solid #4dabf7 !important; /* æ›´ç²—çš„åº•éƒ¨é‚Šæ¡† */
        margin-bottom: 35px !important; /* æ›´å¤§çš„ä¸‹æ–¹é–“è· */
        padding-bottom: 0 !important; /* ç§»é™¤åº•éƒ¨å…§é‚Šè·ä»¥é¿å…é–“éš™ */
    }
    
    /* ç‚ºæ¨™ç±¤é å…§å®¹æ·»åŠ é ‚éƒ¨é–“è·ï¼Œé¿å…èˆ‡æ¨™ç±¤å¤ªè¿‘ */
    div[data-baseweb="tab-panel"] {
        padding-top: 20px !important;
    }
    div[role="tablist"] {
        border-bottom: 3px solid #4dabf7 !important; /* åŠ ç²—åº•éƒ¨é‚Šæ¡† */
        margin-bottom: 30px !important; /* å¢åŠ ä¸‹æ–¹é–“è· */
    }
    
    /* å¡ç‰‡å®¹å™¨æ¨£å¼ */
    .card-container {
        background-color: white;
        padding: 20px;
        border-radius: 10px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
        margin-bottom: 30px;
        border-left: 4px solid #4dabf7;
    }
    
    /* åƒæ•¸çµ„æ¨£å¼ */
    .param-group {
        background-color: #f8f9fa;
        padding: 15px;
        border-radius: 8px;
        margin-bottom: 15px;
    }
    
    /* åƒæ•¸å€å¡Šæ¨£å¼ */
    .parameter-section {
        margin-bottom: 20px;
    }
    
    /* ç‰¹å¾µåˆ—è¡¨å€å¡Šæ¨£å¼ */
    .feature-list-section {
        background-color: #f0f9ff;
        padding: 15px;
        border-radius: 8px;
        margin-bottom: 15px;
        border-left: 3px solid #339af0;
    }
    
    /* ç¾åŒ–æ¨™é¡Œ */
    .stSubheader {
        font-size: 1.5rem !important;
        font-weight: 600 !important;
        color: #2c3e50 !important;
        margin-bottom: 1rem !important;
        padding-bottom: 0.5rem !important;
        border-bottom: 2px solid #f1f3f5 !important;
    }
    
    /* æŒ‡æ¨™å¡ç‰‡æ¨£å¼ */
    div.stMetric {
        background-color: #f8f9fa;
        border-radius: 8px;
        padding: 15px !important;
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
        transition: transform 0.3s;
        border-left: 5px solid #4dabf7;
    }
    div.stMetric:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    
    /* è¡¨æ ¼ç¾åŒ– */
    div.stTable, div[data-testid="stDataFrame"] {
        border-radius: 8px;
        overflow: hidden;
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
    }
    
    /* è¼¸å…¥å…ƒç´ ç¾åŒ– */
    div.stSlider, div.stSelectbox, div.stNumberInput, div.stCheckbox {
        padding: 10px;
        border-radius: 8px;
        background-color: #f8f9fa;
        margin-bottom: 15px;
        border: 1px solid #e9ecef;
    }
    
    /* æŒ‰éˆ•ç¾åŒ– */
    button[kind="primary"], button[kind="secondary"] {
        font-size: 18px !important; /* åŠ å¤§å­—é«” */
        padding: 14px 22px !important; /* å¢å¤§å…§é‚Šè· */
        height: auto !important; /* è‡ªå‹•é«˜åº¦é©æ‡‰å…§å®¹ */
        min-height: 60px !important; /* ç¢ºä¿æœ€å°é«˜åº¦ */
        border-radius: 10px !important; /* å¢å¤§åœ“è§’ */
        transition: all 0.3s !important;
    }
    }
    button[kind="primary"]:hover {
        transform: translateY(-2px) !important; /* æ‡¸åœæ™‚å¾®å¾®ä¸Šæµ® */
        background-color: #339af0 !important;
        box-shadow: 0 6px 12px rgba(0,0,0,0.1) !important;
    }
    button[kind="secondary"] {
        border-radius: 8px !important;
        transition: all 0.3s !important;
    }
    button[kind="secondary"]:hover {
        transform: translateY(-2px) !important;
        background-color: #e9ecef !important;
        box-shadow: 0 6px 12px rgba(0,0,0,0.1) !important;
    }
    
    /* ç‹€æ…‹æŒ‡ç¤ºå™¨ */
    div[data-testid="stAlert"][kind="success"] {
        border-radius: 8px;
        border-left: 5px solid #51cf66;
    }
    div[data-testid="stAlert"][kind="warning"] {
        border-radius: 8px;
        border-left: 5px solid #fcc419;
    }
    div[data-testid="stAlert"][kind="error"] {
        border-radius: 8px;
        border-left: 5px solid #ff6b6b;
    }
    div[data-testid="stAlert"][kind="info"] {
        border-radius: 8px;
        border-left: 5px solid #4dabf7;
    }
</style>
""", unsafe_allow_html=True)

st.markdown(f"""
<div style="background-color: #4dabf7; padding: 20px 28px; display: flex; justify-content: space-between; align-items: center; color: white; margin-bottom: 20px; border-radius: 10px; box-shadow: 0 3px 10px rgba(0,0,0,0.1);">
    <div style="font-size: 34px; font-weight: 800;"> MLP æ¨¡å‹è¨“ç·´èˆ‡é æ¸¬ç³»çµ±</div>
    <div style="text-align: right;">
        <div style="font-size: 20px; opacity: 0.9;">{date_str}</div>
        <div style="font-size: 25px; font-weight: bold;">{time_str}</div>
    </div>
</div>
""", unsafe_allow_html=True)
st.markdown("""
<h4 style="margin-bottom: px; font-weight: normal; color: #555;">
é€éèª¿æ•´åƒæ•¸è¨“ç·´ MLP æ¨¡å‹ï¼Œä¸¦å³æ™‚é€²è¡Œé æ¸¬
</h4>
""", unsafe_allow_html=True)


def create_downloadable_plot(fig, filename="plot.png"):
    """å°‡ matplotlib åœ–å½¢è½‰æ›ç‚ºå¯ä¸‹è¼‰çš„æ ¼å¼"""
    buffer = BytesIO()
    fig.savefig(buffer, format='png', dpi=300, bbox_inches='tight')
    buffer.seek(0)
    return buffer

def plot_decision_boundary(mlp_model, X_train, y_train, feature_indices, feature_names, tatarget_names, scaler, resolution=100):
    """ç¹ªè£½2Dæ±ºç­–é‚Šç•Œ"""
    import matplotlib.patches as mpatches
    
    # ç²å–å…©å€‹ç‰¹å¾µçš„ç¯„åœ
    X_subset = X_train.iloc[:, feature_indices]
    x_min, x_max = X_subset.iloc[:, 0].min() - 0.5, X_subset.iloc[:, 0].max() + 0.5
    y_min, y_max = X_subset.iloc[:, 1].min() - 0.5, X_subset.iloc[:, 1].max() + 0.5
    
    # å‰µå»ºç¶²æ ¼
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, resolution),
                         np.linspace(y_min, y_max, resolution))
    
    # æº–å‚™é æ¸¬æ•¸æ“š
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    
    # å¦‚æœæ¨¡å‹éœ€è¦æ›´å¤šç‰¹å¾µï¼Œç”¨å¹³å‡å€¼å¡«å……
    if len(feature_indices) < X_train.shape[1]:
        # å‰µå»ºä¸€å€‹åŒ…å«æ‰€æœ‰ç‰¹å¾µçš„ DataFrame
        full_grid_data = {}
        
        # é¦–å…ˆï¼Œç‚ºæ‰€æœ‰ç‰¹å¾µè¨­ç½®å¹³å‡å€¼
        for col_idx, col_name in enumerate(X_train.columns):
            full_grid_data[col_name] = np.full(grid_points.shape[0], X_train.iloc[:, col_idx].mean())
        
        # ç„¶å¾Œï¼Œè¦†å¯«é¸å®šçš„å…©å€‹ç‰¹å¾µ
        full_grid_data[X_train.columns[feature_indices[0]]] = grid_points[:, 0]
        full_grid_data[X_train.columns[feature_indices[1]]] = grid_points[:, 1]
        
        # å‰µå»º DataFrame
        full_grid_df = pd.DataFrame(full_grid_data)
        
        # ç¢ºä¿åˆ—çš„é †åºèˆ‡è¨“ç·´æ•¸æ“šä¸€è‡´
        full_grid_df = full_grid_df[X_train.columns]
        
        # é€²è¡Œé æ¸¬
        Z = mlp_model.predict(full_grid_df)
    else:
        # å¦‚æœåªæœ‰å…©å€‹ç‰¹å¾µï¼Œç›´æ¥å‰µå»º DataFrame
        grid_df = pd.DataFrame(grid_points, columns=[X_train.columns[feature_indices[0]], 
                                                     X_train.columns[feature_indices[1]]])
        Z = mlp_model.predict(grid_df)
    
    Z = Z.reshape(xx.shape)
    
    # ç¹ªè£½æ±ºç­–é‚Šç•Œ
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # ä½¿ç”¨æ›´ç¾è§€çš„é¡è‰²
    colors = ['#FFE5E5', '#E5F2FF', '#E5FFE5']  # æ·¡è‰²èƒŒæ™¯
    contour = ax.contourf(xx, yy, Z, alpha=0.6, colors=colors, levels=[-0.5, 0.5, 1.5, 2.5])
    
    # ç¹ªè£½è¨“ç·´æ•¸æ“šé»
    scatter_colors = ['#FF6B6B', '#4DABF7', '#51CF66']  # é®®æ˜çš„é»é¡è‰²
    
    # é€™è£¡éœ€è¦ç¢ºä¿ target_names åœ¨å‡½æ•¸ä½œç”¨åŸŸå…§
    # å‡è¨­ target_names æ˜¯å…¨å±€è®Šé‡ï¼Œå¦‚æœä¸æ˜¯ï¼Œéœ€è¦ä½œç‚ºåƒæ•¸å‚³å…¥
    target_names_local = ['setosa', 'versicolor', 'virginica']  # æˆ–å¾åƒæ•¸ç²å–
    
    for i, (class_name, color) in enumerate(zip(target_names_local, scatter_colors)):
        idx = y_train == i
        ax.scatter(X_subset.iloc[idx, 0], X_subset.iloc[idx, 1], 
                  c=color, label=class_name, edgecolors='black', s=100, alpha=0.8)
    
    ax.set_xlabel(f'{feature_names[0]}ï¼ˆæ¨™æº–åŒ–å¾Œï¼‰')
    ax.set_ylabel(f'{feature_names[1]}ï¼ˆæ¨™æº–åŒ–å¾Œï¼‰')
    ax.set_title('MLP æ±ºç­–é‚Šç•Œè¦–è¦ºåŒ–')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    return fig

# --- æ¨¡å‹ä¿å­˜è·¯å¾‘è¨­å®š ---
MODEL_PATH = "mlp_model.pkl"
SCALER_PATH = "scaler.pkl"

# --- å­—å‹è¨­å®š ---
current_dir = os.path.dirname(__file__)
font_path = os.path.join(current_dir, "fonts", "NotoSansTC-VariableFont_wght.ttf")

if os.path.exists(font_path):
    fontManager.addfont(font_path)
    font_prop = FontProperties(fname=font_path) 
    font_family_name = font_prop.get_name() 
    
    plt.rcParams['font.family'] = font_family_name
    plt.rcParams['font.sans-serif'] = [font_family_name, 'Arial Unicode MS', 'DejaVu Sans', 'sans-serif']
    plt.rcParams['axes.unicode_minus'] = False
else:
    plt.rcParams['axes.unicode_minus'] = False 
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'sans-serif']

# è¨­ç½®å…¨å±€åœ–è¡¨é¢¨æ ¼
plt.style.use('seaborn-v0_8-paper')
    
    # æ›´æ–°åœ–è¡¨åƒæ•¸ç‚ºç ”ç©¶è«–æ–‡é¢¨æ ¼
plt.rcParams.update({
        'figure.facecolor': 'white',
        'axes.facecolor': 'white',
        'axes.edgecolor': 'black',
        'axes.labelcolor': 'black',
        'axes.spines.top': True,
        'axes.spines.right': True,
        'axes.grid': True,
        'grid.color': '#dddddd',
        'grid.linestyle': '--',
        'grid.alpha': 0.7,
        'xtick.color': 'black',
        'ytick.color': 'black',
        'font.size': 11,
        'axes.labelsize': 12,
        'axes.titlesize': 14,
        'lines.linewidth': 1.5,
        
        # è¨­ç½®ä¸­æ–‡å­—é«”æ”¯æ´
        'font.family': font_family_name,
        'font.sans-serif': [font_family_name, 'DejaVu Serif', 'serif'],
        'axes.unicode_minus': False
    })

# --- æ•¸æ“šåŠ è¼‰èˆ‡é è™•ç† (ä½¿ç”¨ Streamlit ç·©å­˜) ---
@st.cache_data
def load_and_preprocess_data(test_size=0.2, use_stratify=True, random_state=42):
    iris = load_iris()
    X = pd.DataFrame(iris.data, columns=iris.feature_names)
    y = iris.target

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled_df = pd.DataFrame(X_scaled, columns=iris.feature_names)
    
    # ä¾æ“šä½¿ç”¨è€…é¸æ“‡æ±ºå®šæ˜¯å¦ä½¿ç”¨åˆ†å±¤æŠ½æ¨£
    stratify_param = y if use_stratify else None
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled_df, y, test_size=test_size, random_state=random_state, stratify=stratify_param
    )
    
    return X_train, X_test, y_train, y_test, iris.target_names, X.columns.tolist(), scaler

# --- è‡ªå®šç¾©è¨“ç·´å‡½æ•¸withçœŸå¯¦é€²åº¦ ---
def train_mlp_with_progress(mlp, X_train, y_train, progress_bar, status_text):
    """è¨“ç·´MLPä¸¦é¡¯ç¤ºé€²åº¦"""
    import time
    import warnings
    
    # çµ±ä¸€ä½¿ç”¨æ¨™æº–è¨“ç·´æ–¹æ³•ï¼Œç¢ºä¿ä¸€è‡´æ€§
    status_text.text("ğŸš€ é–‹å§‹æ¨¡å‹è¨“ç·´...")
    
    # æ¨¡æ“¬è¨“ç·´é€²åº¦
    for i in range(20, 85, 5):
        progress_bar.progress(i/100)
        time.sleep(0.1)
        status_text.text(f"ğŸƒâ€â™‚ï¸ è¨“ç·´é€²åº¦: {i}%")
    
    # å¯¦éš›è¨“ç·´ - æŠ‘åˆ¶æ”¶æ–‚è­¦å‘Š
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", message=".*Stochastic Optimizer.*")
        warnings.filterwarnings("ignore", message=".*Maximum iterations.*")
        mlp.fit(X_train, y_train)
    
    # å®Œæˆé€²åº¦
    progress_bar.progress(0.85)
    status_text.text("âœ… è¨“ç·´å®Œæˆ")
    
    return mlp

# --- è©•ä¼°å‡½æ•¸ ---
def comprehensive_evaluation(mlp, X_train, X_test, y_train, y_test, target_names):
    """ç¶œåˆè©•ä¼°æ¨¡å‹æ€§èƒ½ - ç¢ºä¿æ•¸æ“šä¸€è‡´æ€§"""
    
    # ç¢ºä¿ä½¿ç”¨æ­£ç¢ºçš„ç‰¹å¾µé€²è¡Œé æ¸¬
    y_pred_train = mlp.predict(X_train)
    y_pred_test = mlp.predict(X_test)
    y_pred_proba = mlp.predict_proba(X_test)
    
    # åŸºæœ¬æŒ‡æ¨™
    train_accuracy = accuracy_score(y_train, y_pred_train)
    test_accuracy = accuracy_score(y_test, y_pred_test)
    
    # è©³ç´°åˆ†é¡æŒ‡æ¨™ - ä½¿ç”¨æ¸¬è©¦é›†
    precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred_test, average=None)
    
    # æª¢æŸ¥æ”¶æ–‚ç‹€æ…‹
    convergence_info = {
        'converged': mlp.n_iter_ < mlp.max_iter,
        'actual_iterations': mlp.n_iter_,
        'max_iterations': mlp.max_iter
    }
    
    # äº¤å‰é©—è­‰åˆ†æ•¸ - ä½¿ç”¨èˆ‡è¨“ç·´ç›¸åŒçš„ç‰¹å¾µ
    try:
        # å‰µå»ºæ–°çš„æ¨¡å‹å¯¦ä¾‹é€²è¡Œäº¤å‰é©—è­‰ï¼Œé¿å…å½±éŸ¿å·²è¨“ç·´çš„æ¨¡å‹
        cv_model = MLPClassifier(
            hidden_layer_sizes=mlp.hidden_layer_sizes,
            activation=mlp.activation,
            solver=mlp.solver,
            alpha=mlp.alpha,
            batch_size=mlp.batch_size,
            learning_rate=mlp.learning_rate,
            learning_rate_init=mlp.learning_rate_init,
            max_iter=mlp.max_iter,
            early_stopping=mlp.early_stopping,
            validation_fraction=mlp.validation_fraction,
            n_iter_no_change=mlp.n_iter_no_change,
            tol=mlp.tol,
            random_state=42,
            verbose=False
        )
        
        # ä½¿ç”¨ç›¸åŒçš„è¨“ç·´æ•¸æ“šå’Œæ¨™ç±¤é€²è¡Œäº¤å‰é©—è­‰
        import warnings
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")  # æš«æ™‚å¿½ç•¥äº¤å‰é©—è­‰ä¸­çš„æ”¶æ–‚è­¦å‘Š
            cv_scores = cross_val_score(cv_model, X_train, y_train, cv=5, scoring='accuracy')
        
    except Exception as e:
        # å¦‚æœäº¤å‰é©—è­‰å¤±æ•—ï¼Œä½¿ç”¨æ¸¬è©¦æº–ç¢ºç‡ä½œç‚ºæ›¿ä»£
        print(f"äº¤å‰é©—è­‰å¤±æ•—: {e}")
        cv_scores = np.array([test_accuracy] * 5)  # ä½¿ç”¨æ¸¬è©¦æº–ç¢ºç‡å¡«å……
    
    return {
        'y_pred_train': y_pred_train,
        'y_pred_test': y_pred_test,
        'y_pred_proba': y_pred_proba,
        'train_accuracy': train_accuracy,
        'test_accuracy': test_accuracy,
        'cv_scores': cv_scores,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'support': support,
        'convergence_info': convergence_info
    }


# --- å´é‚Šæ¬„åƒæ•¸è¨­å®š ---
st.sidebar.header('ğŸ”§ MLP æ¨¡å‹è¶…åƒæ•¸è¨­å®š')

# è³‡æ–™é›†åˆ‡åˆ†è¨­å®š - å…ˆå®šç¾©é€™äº›è®Šæ•¸
st.sidebar.subheader('ğŸ“Š è³‡æ–™é›†åˆ‡åˆ†è¨­å®š')
test_size = st.sidebar.slider('æ¸¬è©¦é›†æ¯”ä¾‹', 0.1, 0.5, 0.2, step=0.05, 
                         help="è¨­å®šç”¨æ–¼æ¸¬è©¦çš„è³‡æ–™æ¯”ä¾‹ï¼Œä¸€èˆ¬å»ºè­°åœ¨ 0.1~0.3 ä¹‹é–“")
use_stratify = st.sidebar.checkbox('å•Ÿç”¨åˆ†å±¤æŠ½æ¨£', value=True, 
                             help="ç¢ºä¿è¨“ç·´é›†å’Œæ¸¬è©¦é›†ä¸­å„é¡åˆ¥æ¯”ä¾‹ä¸€è‡´")
random_state = st.sidebar.number_input('éš¨æ©Ÿç¨®å­', min_value=0, max_value=100, value=42, step=1, 
                                  help="æ§åˆ¶è³‡æ–™åˆ‡åˆ†çš„éš¨æ©Ÿæ€§ï¼Œè¨­å®šå›ºå®šå€¼å¯ç¢ºä¿çµæœå¯é‡ç¾")

# åˆå§‹åŒ–æ‰€æœ‰ session state
if 'iris_original_data_loaded' not in st.session_state:
    iris_full_dataset = load_iris()
    st.session_state.original_X_df = pd.DataFrame(iris_full_dataset.data, columns=iris_full_dataset.feature_names)
    st.session_state.original_y = iris_full_dataset.target
    if 'target_names' not in st.session_state:
        st.session_state.target_names = iris_full_dataset.target_names.copy()
    st.session_state.iris_original_data_loaded = True

if 'model_trained' not in st.session_state:
    st.session_state.model_trained = False
if 'training_results' not in st.session_state:
    st.session_state.training_results = None
if 'last_selected_features' not in st.session_state:
    st.session_state.last_selected_features = None
if 'last_split_params' not in st.session_state:
    st.session_state.last_split_params = (test_size, use_stratify, random_state)
elif st.session_state.last_split_params != (test_size, use_stratify, random_state):
    st.session_state.model_trained = False
    st.session_state.training_results = None
    st.sidebar.warning("âš ï¸ è³‡æ–™é›†åˆ‡åˆ†åƒæ•¸å·²è®Šæ›´ï¼Œéœ€è¦é‡æ–°è¨“ç·´æ¨¡å‹")

st.session_state.last_split_params = (test_size, use_stratify, random_state)

# ç¾åœ¨å¯ä»¥å®‰å…¨åœ°åŠ è¼‰æ•¸æ“šï¼Œå› ç‚ºæ‰€æœ‰éœ€è¦çš„è®Šæ•¸éƒ½å·²ç¶“å®šç¾©
X_train_full, X_test_full, y_train, y_test, target_names, all_feature_names, loaded_scaler = load_and_preprocess_data(
    test_size=test_size,
    use_stratify=use_stratify,
    random_state=random_state
)

# ç¹¼çºŒå…¶ä»–å´é‚Šæ¬„è¨­å®š
st.sidebar.subheader('ğŸ“Š ç‰¹å¾µé¸æ“‡')
selected_features = st.sidebar.multiselect(
    'é¸æ“‡è¦åŒ…å«çš„ç‰¹å¾µ',
    options=all_feature_names,
    default=all_feature_names
)

# æª¢æŸ¥ç‰¹å¾µæ˜¯å¦æ”¹è®Š
if (st.session_state.last_selected_features is not None and 
    st.session_state.last_selected_features != selected_features):
    st.session_state.model_trained = False
    st.session_state.training_results = None
    
st.session_state.last_selected_features = selected_features

if not selected_features:
    st.sidebar.warning("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹ç‰¹å¾µï¼")
    st.error("âš ï¸ è«‹åœ¨å·¦å´é‚Šæ¬„è‡³å°‘é¸æ“‡ä¸€å€‹ç‰¹å¾µæ‰èƒ½ç¹¼çºŒ")
    st.stop()

# æ ¹æ“šé¸æ“‡çš„ç‰¹å¾µç¯©é¸æ•¸æ“šé›†
X_train = X_train_full[selected_features]
X_test = X_test_full[selected_features]

# æ¨¡å‹è¤‡é›œåº¦
st.sidebar.subheader('ğŸ—ï¸ æ¨¡å‹è¤‡é›œåº¦')
hidden_layer_1 = st.sidebar.slider('ç¬¬ä¸€éš±è—å±¤ç¥ç¶“å…ƒæ•¸é‡', 10, 200, 100, step=10)
num_hidden_layers = st.sidebar.radio('éš±è—å±¤æ•¸é‡', [1, 2], index=0)

hidden_layer_sizes = (hidden_layer_1,)
if num_hidden_layers == 2:
    hidden_layer_2 = st.sidebar.slider('ç¬¬äºŒéš±è—å±¤ç¥ç¶“å…ƒæ•¸é‡', 10, 100, 50, step=10)
    hidden_layer_sizes = (hidden_layer_1, hidden_layer_2)

st.sidebar.write(f'ğŸ”¹ éš±è—å±¤çµæ§‹: {hidden_layer_sizes}')

# æ´»åŒ–å‡½æ•¸
activation_function = st.sidebar.selectbox(
    'âš¡ æ´»åŒ–å‡½æ•¸',
    ['relu', 'logistic', 'tanh', 'identity'],
    index=0
)

# å„ªåŒ–å™¨
solver = st.sidebar.selectbox(
    'ğŸš€ å„ªåŒ–å™¨',
    ['adam', 'sgd', 'lbfgs'],
    index=0
)

# å­¸ç¿’ç‡è¨­å®š
learning_rate_init = 0.001
if solver in ['adam', 'sgd']:
    st.sidebar.subheader('ğŸ“ˆ å­¸ç¿’ç‡è¨­å®š')
    learning_rate = st.sidebar.selectbox(
        'å­¸ç¿’ç‡ç­–ç•¥',
        ['constant', 'invscaling', 'adaptive'],
        index=0
    )
    learning_rate_init = st.sidebar.number_input('åˆå§‹å­¸ç¿’ç‡', min_value=0.0001, max_value=0.1, value=0.001, step=0.0001, format="%.4f")
else:
    learning_rate = 'constant'

# æ‰¹æ¬¡å¤§å°
batch_size = 'auto'
if solver in ['adam', 'sgd']:
    st.sidebar.subheader('ğŸ“¦ æ‰¹æ¬¡å¤§å°')
    batch_size_option = st.sidebar.radio('æ‰¹æ¬¡å¤§å°è¨­å®š', ['auto', 'æ‰‹å‹•è¼¸å…¥'], index=0)
    if batch_size_option == 'æ‰‹å‹•è¼¸å…¥':
        batch_size = st.sidebar.number_input('Batch Size', min_value=1, max_value=len(X_train), value=32, step=1)

# è¨“ç·´åƒæ•¸
st.sidebar.subheader('ğŸ¯ è¨“ç·´åƒæ•¸')
max_iter = st.sidebar.slider('æœ€å¤§è¿­ä»£æ¬¡æ•¸', 50, 1000, 200, step=50)

early_stopping = st.sidebar.checkbox('å•Ÿç”¨ Early Stopping', value=False)
validation_fraction = 0.1
n_iter_no_change = 10
tol = 1e-4

if early_stopping:
    validation_fraction = st.sidebar.slider('é©—è­‰é›†æ¯”ä¾‹', 0.05, 0.5, 0.1, step=0.05)
    n_iter_no_change = st.sidebar.slider('ç„¡æ”¹å–„å®¹å¿è¿­ä»£æ¬¡æ•¸', 10, 100, 10, step=5)
    tol = st.sidebar.number_input('å®¹å¿åº¦', min_value=1e-5, max_value=1e-2, value=1e-4, format="%.5f")

# æ­£è¦åŒ–åƒæ•¸
alpha = st.sidebar.number_input('ğŸ›¡ï¸ L2 æ­£å‰‡åŒ–å¼·åº¦', min_value=0.0001, max_value=1.0, value=0.0001, step=0.0001, format="%.4f")

# æª¢æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„æ¨¡å‹
model_exists = os.path.exists(MODEL_PATH) and os.path.exists(SCALER_PATH)
if model_exists:
    st.sidebar.success("âœ… ç™¼ç¾å·²ä¿å­˜çš„æ¨¡å‹")
    if not st.session_state.model_trained or st.session_state.training_results is None:
        try:
            # å˜—è©¦è¼‰å…¥æ¨¡å‹
            loaded_mlp_model = joblib.load(MODEL_PATH)
            loaded_data_scaler = joblib.load(SCALER_PATH)
            
            # åœ¨è¼‰å…¥é é¢æ™‚åŸ·è¡Œç°¡å–®è©•ä¼°ç²å–åŸºæœ¬æŒ‡æ¨™
            evaluation_results = comprehensive_evaluation(
                loaded_mlp_model, X_train, X_test, y_train, y_test, target_names
            )
            
            # æ›´æ–°session state
            st.session_state.training_results = {
                'mlp': loaded_mlp_model,
                'selected_features': selected_features,
                **evaluation_results
            }
            st.session_state.model_trained = True
            
        except Exception as e:
            st.sidebar.warning(f"âš ï¸ ç™¼ç¾æ¨¡å‹æ–‡ä»¶ä½†ç„¡æ³•è¼‰å…¥: {e}")
else:
    st.sidebar.info("â„¹ï¸ å°šæœªè¨“ç·´æ¨¡å‹")




# å¢åŠ é–“éš”ï¼Œä½¿æ”¶æ–‚å„ªåŒ–éƒ¨åˆ†èˆ‡å‰é¢çš„å»ºè­°æœ‰æ‰€å€åˆ†
st.sidebar.markdown("&nbsp;")  # ç©ºç™½é–“éš”

# ç•¶å‰ç‹€æ…‹é¡¯ç¤º
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ“‹ ç•¶å‰ç‹€æ…‹")

if st.sidebar.checkbox("é¡¯ç¤ºç‹€æ…‹èª¿è©¦ä¿¡æ¯", False):
    st.sidebar.write(f"æ¨¡å‹æ–‡ä»¶å­˜åœ¨: {os.path.exists(MODEL_PATH)}")

if st.session_state.model_trained and st.session_state.training_results:
    results = st.session_state.training_results
    st.sidebar.success("âœ… æ¨¡å‹å·²è¨“ç·´")
    st.sidebar.metric("æ¸¬è©¦æº–ç¢ºç‡", f"{results['test_accuracy']:.3f}")
    st.sidebar.metric("F1-Score", f"{results['f1'].mean():.3f}")
else:
    st.sidebar.info("â³ ç­‰å¾…è¨“ç·´")

# æ•¸æ“šé›†ä¿¡æ¯
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ“Š æ•¸æ“šé›†è³‡è¨Š")
st.sidebar.write(f"â€¢ ç¸½æ¨£æœ¬æ•¸: {len(X_train_full) + len(X_test_full)}")
st.sidebar.write(f"â€¢ è¨“ç·´é›†: {len(X_train_full)} æ¨£æœ¬")
st.sidebar.write(f"â€¢ æ¸¬è©¦é›†: {len(X_test_full)} æ¨£æœ¬")
st.sidebar.write(f"â€¢ ç‰¹å¾µç¸½æ•¸: {len(all_feature_names)}")
st.sidebar.write(f"â€¢ é¸æ“‡ç‰¹å¾µ: {len(selected_features)}")
st.sidebar.write(f"â€¢ é¡åˆ¥æ•¸: {len(target_names)}")
st.sidebar.markdown("&nbsp;")  # ç©ºç™½é–“éš”

# åˆ‡åˆ†æ¯”ä¾‹å»ºè­°
st.sidebar.markdown("**ğŸ’¡ åˆ‡åˆ†æ¯”ä¾‹å»ºè­°ï¼š**")
total_samples = len(X_train_full) + len(X_test_full)

if total_samples < 100:
    st.sidebar.info(f"å°å‹è³‡æ–™é›† ({total_samples} æ¨£æœ¬)ï¼Œå»ºè­°æ¸¬è©¦é›†æ¯”ä¾‹: 0.2~0.3")
elif total_samples < 1000:
    st.sidebar.info(f"ä¸­å‹è³‡æ–™é›† ({total_samples} æ¨£æœ¬)ï¼Œå»ºè­°æ¸¬è©¦é›†æ¯”ä¾‹: 0.15~0.25")
else:
    st.sidebar.info(f"å¤§å‹è³‡æ–™é›† ({total_samples} æ¨£æœ¬)ï¼Œå»ºè­°æ¸¬è©¦é›†æ¯”ä¾‹: 0.1~0.2")

# è­¦å‘Šéå°çš„æ¸¬è©¦é›†
min_test_samples = int(total_samples * test_size)
if min_test_samples < 30:
    st.sidebar.warning(f"âš ï¸ æ¸¬è©¦é›†åƒ…æœ‰ {min_test_samples} å€‹æ¨£æœ¬ï¼Œå¯èƒ½ä¸è¶³ä»¥å¯é è©•ä¼°æ¨¡å‹")
st.sidebar.markdown("&nbsp;")  # ç©ºç™½é–“éš”
# åƒæ•¸å»ºè­°
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ’¡ åƒæ•¸èª¿å„ªå»ºè­°")

if len(selected_features) < 4:
    st.sidebar.warning("ç‰¹å¾µæ•¸é‡è¼ƒå°‘ï¼Œå»ºè­°ä½¿ç”¨è¼ƒå°çš„éš±è—å±¤")
    
if solver == 'lbfgs' and max_iter > 500:
    st.sidebar.info("L-BFGS é€šå¸¸æ”¶æ–‚è¼ƒå¿«ï¼Œå¯å˜—è©¦è¼ƒå°‘è¿­ä»£æ¬¡æ•¸")
elif solver in ['adam', 'sgd'] and max_iter < 500:
    st.sidebar.warning("Adam/SGD å¯èƒ½éœ€è¦æ›´å¤šè¿­ä»£æ¬¡æ•¸é¿å…æ”¶æ–‚è­¦å‘Š")
    
if solver in ['adam', 'sgd'] and learning_rate_init > 0.01:
    st.sidebar.warning("å­¸ç¿’ç‡è¼ƒé«˜ï¼Œå¯èƒ½å°è‡´è¨“ç·´ä¸ç©©å®šæˆ–é›£ä»¥æ”¶æ–‚")

if hidden_layer_1 > 150 and len(selected_features) <= 4:
    st.sidebar.warning("éš±è—å±¤ç¥ç¶“å…ƒæ•¸å¯èƒ½éå¤šï¼Œæ˜“éæ“¬åˆ")
# æ”¶æ–‚å„ªåŒ–å»ºè­°
st.sidebar.markdown("**ğŸ”„ æ”¶æ–‚å„ªåŒ–ï¼š**")
st.sidebar.write("â€¢ é‡åˆ°æ”¶æ–‚è­¦å‘Šæ™‚å¢åŠ è¿­ä»£æ¬¡æ•¸")
st.sidebar.write("â€¢ å•Ÿç”¨ Early Stopping é˜²æ­¢éåº¦è¨“ç·´")
st.sidebar.write("â€¢ èª¿ä½å­¸ç¿’ç‡æé«˜ç©©å®šæ€§")
# --- ä¸»è¦å…§å®¹å€åŸŸä½¿ç”¨ Tabs ---
# ä½¿ç”¨æ›´ç¾è§€çš„æ¨™ç±¤é 
tabs = st.tabs([
    "ğŸ¯ **æ¨¡å‹è¨“ç·´** ", 
    "ğŸ“Š **è¨“ç·´çµæœ** ", 
    "ğŸ”® **å³æ™‚é æ¸¬** "
])

# --- Tab 1: æ¨¡å‹è¨“ç·´ ---
with tabs[0]:
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown('<div class="card-container">', unsafe_allow_html=True)
        st.subheader("ğŸ”§ ç•¶å‰æ¨¡å‹è¨­å®š")
        
        # åƒæ•¸å±•ç¤º
        param_col1, param_col2 = st.columns(2)
        with param_col1:
            st.write("**ğŸ—ï¸ æ¨¡å‹çµæ§‹**")
            st.write(f"â€¢ ç‰¹å¾µæ•¸é‡: {len(selected_features)}")
            st.write(f"â€¢ éš±è—å±¤: {hidden_layer_sizes}")
            st.write(f"â€¢ æ´»åŒ–å‡½æ•¸: {activation_function}")
            st.write(f"â€¢ å„ªåŒ–å™¨: {solver}")
            st.write("**ğŸ“Š è³‡æ–™é›†åˆ‡åˆ†è³‡è¨Šï¼š**")
            st.write(f"â€¢ æ¸¬è©¦é›†æ¯”ä¾‹: {test_size:.0%}")
            st.write(f"â€¢ åˆ†å±¤æŠ½æ¨£: {'âœ… å·²å•Ÿç”¨' if use_stratify else 'âŒ æœªå•Ÿç”¨'}")
            st.write(f"â€¢ éš¨æ©Ÿç¨®å­: {random_state}")
            st.write(f"â€¢ è¨“ç·´é›†å¤§å°: {len(X_train)} æ¨£æœ¬")
            st.write(f"â€¢ æ¸¬è©¦é›†å¤§å°: {len(X_test)} æ¨£æœ¬")

            # é¡¯ç¤ºå„é¡åˆ¥åœ¨è¨“ç·´é›†ä¸­çš„åˆ†å¸ƒ
            train_class_dist = pd.Series(y_train).value_counts().sort_index()
            train_class_dist.index = [target_names[i] for i in train_class_dist.index]
            st.write("â€¢ è¨“ç·´é›†é¡åˆ¥åˆ†å¸ƒ: ", dict(train_class_dist))

        with param_col2:
            st.write("**âš™ï¸ è¨“ç·´åƒæ•¸**")
            st.write(f"â€¢ æœ€å¤§è¿­ä»£æ¬¡æ•¸: {max_iter}")
            st.write(f"â€¢ Early Stopping: {'å•Ÿç”¨' if early_stopping else 'åœç”¨'}")
            st.write(f"â€¢ L2 æ­£å‰‡åŒ–: {alpha}")
            if solver in ['adam', 'sgd']:
                st.write(f"â€¢ å­¸ç¿’ç‡: {learning_rate_init}")
        
        st.write("**ğŸ“‹ é¸æ“‡çš„ç‰¹å¾µ:**")
        st.write(", ".join(selected_features))
        st.markdown('</div>', unsafe_allow_html=True)

    with col2:
        st.markdown('<div class="card-container">', unsafe_allow_html=True)
        st.subheader("ğŸš€ æ¨¡å‹æ§åˆ¶")
        
        train_col, reset_col = st.columns(2)
        
        with train_col:
            if st.button('ğŸ¯ è¨“ç·´æ¨¡å‹', type="primary", use_container_width=True):
                if not selected_features:
                    st.error("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹ç‰¹å¾µæ‰èƒ½è¨“ç·´æ¨¡å‹ï¼")
                else:
                    try:
                        # å»ºç«‹é€²åº¦è¿½è¹¤å®¹å™¨
                        progress_container = st.container()
                        with progress_container:
                            progress_bar = st.progress(0)
                            status_text = st.empty()
                        
                        status_text.text("ğŸ”§ å»ºç«‹æ¨¡å‹æ¶æ§‹...")
                        progress_bar.progress(10)
                        
                        # æ•¸æ“šé©—è­‰
                        st.write(f"ğŸ“Š **è¨“ç·´æ•¸æ“šè³‡è¨Š:**")
                        st.write(f"â€¢ é¸æ“‡ç‰¹å¾µ: {len(selected_features)} å€‹")
                        st.write(f"â€¢ è¨“ç·´æ¨£æœ¬: {X_train.shape[0]} å€‹")
                        st.write(f"â€¢ æ¸¬è©¦æ¨£æœ¬: {X_test.shape[0]} å€‹")
                        st.write(f"â€¢ æ¸¬è©¦é›†æ¯”ä¾‹: {test_size:.0%}")
                        st.write(f"â€¢ åˆ†å±¤æŠ½æ¨£: {'å·²å•Ÿç”¨' if use_stratify else 'æœªå•Ÿç”¨'}")
                        st.write(f"â€¢ é¡åˆ¥åˆ†å¸ƒ: {dict(zip(target_names, np.bincount(y_train)))}")
                                                
                        # å»ºç«‹æ¨¡å‹
                        mlp = MLPClassifier(
                            hidden_layer_sizes=hidden_layer_sizes,
                            activation=activation_function,
                            solver=solver,
                            alpha=alpha,
                            batch_size=batch_size,
                            learning_rate=learning_rate,
                            learning_rate_init=learning_rate_init,
                            max_iter=max_iter,
                            early_stopping=early_stopping,
                            validation_fraction=validation_fraction,
                            n_iter_no_change=n_iter_no_change,
                            tol=tol,
                            random_state=42,
                            verbose=False  # é—œé–‰è©³ç´°è¼¸å‡ºï¼Œä½¿ç”¨è‡ªå®šç¾©é€²åº¦
                        )
                        
                        progress_bar.progress(20)
                        
                        # ä½¿ç”¨è‡ªå®šç¾©è¨“ç·´å‡½æ•¸
                        mlp = train_mlp_with_progress(mlp, X_train, y_train, progress_bar, status_text)
                        
                        status_text.text("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
                        progress_bar.progress(90)
                        
                        # ä¿å­˜æ¨¡å‹
                        joblib.dump(mlp, MODEL_PATH)
                        joblib.dump(loaded_scaler, SCALER_PATH)
                        
                        status_text.text("ğŸ“Š ç¶œåˆè©•ä¼°æ¨¡å‹...")
                        progress_bar.progress(95)
                        
                        # ç¶œåˆè©•ä¼°
                        evaluation_results = comprehensive_evaluation(mlp, X_train, X_test, y_train, y_test, target_names)
                        
                        # ä¿å­˜çµæœåˆ° session state
                        st.session_state.training_results = {
                            'mlp': mlp,
                            'selected_features': selected_features,
                            **evaluation_results
                        }
                        st.session_state.model_trained = True
                        # æ·»åŠ é€™å€‹æ¨™è¨˜ï¼Œè¡¨ç¤ºæˆ‘å€‘éœ€è¦åœ¨ä¸‹ä¸€æ¬¡åŸ·è¡Œæ™‚å¼·åˆ¶æ›´æ–°UI
                        st.session_state.need_ui_update = True
                        
                        progress_bar.progress(100)
                        status_text.text("âœ… è¨“ç·´å®Œæˆï¼")
                        
                        # é¡¯ç¤ºå¿«é€Ÿæ‘˜è¦
                        st.success("ğŸ‰ æ¨¡å‹è¨“ç·´æˆåŠŸï¼")
                        st.info("ğŸ’¡ åˆ‡æ›åˆ°ã€ŒğŸ“Š è¨“ç·´çµæœã€æŸ¥çœ‹è©³ç´°åˆ†æï¼Œæˆ–åˆ°ã€ŒğŸ”® å³æ™‚é æ¸¬ã€é€²è¡Œé æ¸¬ã€‚")
                        
                    except Exception as e:
                        st.error(f"âŒ è¨“ç·´éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                        import traceback
                        st.text("è©³ç´°éŒ¯èª¤ä¿¡æ¯ï¼š")
                        st.code(traceback.format_exc())
        
        with reset_col:
            if st.button('ğŸ”„ é‡ç½®æ¨¡å‹', use_container_width=True):
                # æ›´æ–°ç‹€æ…‹
                st.session_state.model_trained = False
                st.session_state.training_results = None
                
                # åˆªé™¤æ¨¡å‹æ–‡ä»¶
                try:
                    if os.path.exists(MODEL_PATH):
                        os.remove(MODEL_PATH)
                    if os.path.exists(SCALER_PATH):
                        os.remove(SCALER_PATH)
                    
                    # åƒ…é¡¯ç¤ºé‡ç½®æˆåŠŸè¨Šæ¯ï¼Œä¸æåŠåˆ·æ–°
                    st.success("âœ… æ¨¡å‹å·²é‡ç½®ï¼")
                    
                    # ç«‹å³ä½¿ç”¨JavaScriptåˆ·æ–°é é¢ï¼Œä¸é¡¯ç¤º"æ­£åœ¨åˆ·æ–°"
                    st.markdown("""
                    <script>
                        // ç«‹å³åˆ·æ–°é é¢
                        window.location.reload();
                    </script>
                    """, unsafe_allow_html=True)
                except:
                    # é¡¯ç¤ºéŒ¯èª¤è¨Šæ¯
                    st.error("âš ï¸ æ¨¡å‹æ–‡ä»¶åˆªé™¤å¤±æ•—ï¼Œä½†ç‹€æ…‹å·²æ¸…é™¤")  
                
        st.markdown('</div>', unsafe_allow_html=True)
    
    # åœ¨æ‰€æœ‰ columns å¤–é¢é¡¯ç¤ºå¿«é€Ÿçµæœé è¦½
    if st.session_state.model_trained and st.session_state.training_results:
        st.markdown('<div class="card-container">', unsafe_allow_html=True)
        st.subheader("ğŸ“Š å¿«é€Ÿçµæœé è¦½")
        preview_col1, preview_col2, preview_col3 = st.columns(3)
        
        evaluation_results = st.session_state.training_results
        
        with preview_col1:
            st.metric("ğŸ¯ æ¸¬è©¦æº–ç¢ºç‡", f"{evaluation_results['test_accuracy']:.3f}")
        with preview_col2:
            st.metric("ğŸ“Š äº¤å‰é©—è­‰", f"{evaluation_results['cv_scores'].mean():.3f}")
        with preview_col3:
            overfitting = evaluation_results['train_accuracy'] - evaluation_results['test_accuracy']
            st.metric("ğŸ” éæ“¬åˆç¨‹åº¦", f"{overfitting:.3f}")
        
        # çµæœåˆç†æ€§æª¢æŸ¥
        if evaluation_results['test_accuracy'] < 0.6:
            st.warning("âš ï¸ æ¸¬è©¦æº–ç¢ºç‡è¼ƒä½ï¼Œå»ºè­°èª¿æ•´è¶…åƒæ•¸æˆ–æª¢æŸ¥ç‰¹å¾µé¸æ“‡")
        
        if abs(evaluation_results['cv_scores'].mean() - evaluation_results['test_accuracy']) > 0.2:
            st.warning("âš ï¸ äº¤å‰é©—è­‰èˆ‡æ¸¬è©¦çµæœå·®ç•°è¼ƒå¤§ï¼Œå¯èƒ½å­˜åœ¨æ•¸æ“šæ´©éœ²æˆ–éæ“¬åˆ")
        st.markdown('</div>', unsafe_allow_html=True)

# --- Tab 2: è¨“ç·´çµæœ ---
with tabs[1]:
    # æª¢æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    model_file_exists = os.path.exists(MODEL_PATH) and os.path.exists(SCALER_PATH)
    if 'model_trained' in st.session_state and st.session_state.model_trained and 'training_results' in st.session_state and st.session_state.training_results:
        # æ­£å¸¸é¡¯ç¤ºçµæœ
        results = st.session_state.training_results
        
        # === æ¨¡å‹æ€§èƒ½ç¸½è¦½ ===
        st.markdown('<div class="card-container">', unsafe_allow_html=True)
        st.subheader("ğŸ¯ æ¨¡å‹æ€§èƒ½ç¸½è¦½")
        
        # ä¸»è¦æŒ‡æ¨™
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "ğŸ¯ æ¸¬è©¦æº–ç¢ºç‡", 
                f"{results['test_accuracy']:.3f}",
                delta=f"{(results['test_accuracy'] - 0.33):.3f}"
            )
            
        with col2:
            cv_mean = results['cv_scores'].mean()
            cv_std = results['cv_scores'].std()
            st.metric(
                "ğŸ“Š äº¤å‰é©—è­‰", 
                f"{cv_mean:.3f}",
                delta=f"Â±{cv_std:.3f}"
            )
            
        with col3:
            overfitting = results['train_accuracy'] - results['test_accuracy']
            overfitting_status = "ğŸŸ¢ æ­£å¸¸" if abs(overfitting) < 0.05 else ("ğŸŸ¡ è¼•å¾®" if overfitting < 0.15 else "ğŸ”´ åš´é‡")
            st.metric(
                "ğŸ” éæ“¬åˆæª¢æ¸¬", 
                f"{overfitting:.3f}",
                delta=overfitting_status
            )
            
        with col4:
            f1_macro = results['f1'].mean()
            f1_status = "ğŸŸ¢ å„ªç§€" if f1_macro > 0.9 else ("ğŸŸ¡ è‰¯å¥½" if f1_macro > 0.7 else "ğŸ”´ éœ€æ”¹é€²")
            st.metric(
                "âš¡ F1-Score", 
                f"{f1_macro:.3f}",
                delta=f1_status
            )
        st.markdown('</div>', unsafe_allow_html=True)
        
        # === æ€§èƒ½è¨ºæ–· ===
        st.markdown('<div class="card-container">', unsafe_allow_html=True)
        st.subheader("ğŸ”§ æ¨¡å‹å¥åº·æª¢æŸ¥")
        
        # ä½¿ç”¨ä¸‰æ¬„å¸ƒå±€
        diag_col1, diag_col2, diag_col3 = st.columns(3)
        
        with diag_col1:
            # æ”¶æ–‚ç‹€æ…‹æª¢æŸ¥
            convergence = results['convergence_info']
            if convergence['converged']:
                st.success(f"âœ… æ¨¡å‹å·²æ”¶æ–‚ ({convergence['actual_iterations']}/{convergence['max_iterations']} è¿­ä»£)")
            else:
                st.error(f"âŒ æ¨¡å‹æœªæ”¶æ–‚ ({convergence['actual_iterations']}/{convergence['max_iterations']} è¿­ä»£)")
        
        with diag_col2:
            # æº–ç¢ºç‡æª¢æŸ¥
            if results['test_accuracy'] > 0.9:
                st.success("âœ… æº–ç¢ºç‡å„ªç§€")
            elif results['test_accuracy'] > 0.7:
                st.info("â„¹ï¸ æº–ç¢ºç‡è‰¯å¥½")
            else:
                st.error("âŒ æº–ç¢ºç‡åä½")
        
        with diag_col3:
            # éæ“¬åˆæª¢æŸ¥
            if abs(overfitting) < 0.05:
                st.success("âœ… ç„¡æ˜é¡¯éæ“¬åˆ")
            elif overfitting > 0.15:
                st.warning("âš ï¸ å¯èƒ½å­˜åœ¨éæ“¬åˆ")
            elif overfitting < -0.05:
                st.warning("âš ï¸ ç•°å¸¸ï¼šæ¸¬è©¦é›†è¡¨ç¾å„ªæ–¼è¨“ç·´é›†")
        st.markdown('</div>', unsafe_allow_html=True)
        
        # === åˆ‡åˆ†ç­–ç•¥èˆ‡äº¤å‰é©—è­‰æ¯”è¼ƒ ===
        st.markdown('<div class="card-container">', unsafe_allow_html=True)
        st.subheader("ğŸ”„ åˆ‡åˆ†ç­–ç•¥èˆ‡äº¤å‰é©—è­‰æ¯”è¼ƒ")

        col1, col2 = st.columns(2)

        with col1:
            st.write("**ğŸ“Š åˆ‡åˆ†è©•ä¼°çµæœ**")
            st.metric("è¨“ç·´é›†æº–ç¢ºç‡", f"{results['train_accuracy']:.3f}")
            st.metric("æ¸¬è©¦é›†æº–ç¢ºç‡", f"{results['test_accuracy']:.3f}")
            st.write(f"æ¸¬è©¦é›†å¤§å°: {len(X_test)} æ¨£æœ¬ ({test_size:.0%})")
            
        with col2:
            st.write("**ğŸ”„ äº¤å‰é©—è­‰çµæœ**")
            cv_mean = results['cv_scores'].mean()
            cv_std = results['cv_scores'].std()
            st.metric("å¹³å‡æº–ç¢ºç‡", f"{cv_mean:.3f}")
            st.metric("æ¨™æº–å·®", f"{cv_std:.3f}")
            st.write(f"äº¤å‰é©—è­‰æŠ˜æ•¸: 5")

        # å°æ¯”åœ–è¡¨
        fig, ax = plt.subplots(figsize=(10, 6))
        x = ['è¨“ç·´é›†', 'æ¸¬è©¦é›†', 'äº¤å‰é©—è­‰']
        y = [results['train_accuracy'], results['test_accuracy'], cv_mean]
        colors = ['#4DABF7', '#FF6B6B', '#51CF66']

        bars = ax.bar(x, y, color=colors, alpha=0.7)
        ax.set_ylim(0, 1.1)
        ax.set_ylabel('æº–ç¢ºç‡')
        ax.set_title('ä¸åŒè©•ä¼°æ–¹æ³•çš„æº–ç¢ºç‡æ¯”è¼ƒ')
        ax.grid(True, alpha=0.3, axis='y')

        for bar, val in zip(bars, y):
            ax.text(bar.get_x() + bar.get_width()/2, val + 0.02, f'{val:.3f}', 
                    ha='center', va='bottom', fontweight='bold')

        st.pyplot(fig)

        # æ·»åŠ ä¸‹è¼‰æŒ‰éˆ•
        buffer = create_downloadable_plot(fig, "evaluation_comparison.png")
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰è©•ä¼°æ¯”è¼ƒåœ–",
            data=buffer,
            file_name="evaluation_comparison.png",
            mime="image/png"
        )
        plt.close(fig)
        st.markdown('</div>', unsafe_allow_html=True)

        # === è©³ç´°è©•ä¼°æŒ‡æ¨™ ===
        st.markdown('<div class="card-container">', unsafe_allow_html=True)
        st.subheader("ğŸ“ˆ è©³ç´°è©•ä¼°æŒ‡æ¨™")
        
        # æ”¾å¤§çš„äº¤å‰é©—è­‰åœ–
        st.write("**ğŸ² äº¤å‰é©—è­‰åˆ†æ•¸åˆ†æ**")
        cv_df = pd.DataFrame({
            'Fold': [f'Fold {i+1}' for i in range(len(results['cv_scores']))],
            'æº–ç¢ºç‡': results['cv_scores']
        })
        
        # å¢å¤§åœ–è¡¨å°ºå¯¸
        fig_cv, ax_cv = plt.subplots(figsize=(12, 6))
        bars = ax_cv.bar(cv_df['Fold'], cv_df['æº–ç¢ºç‡'], color='skyblue', alpha=0.7, width=0.6)
        ax_cv.axhline(y=cv_df['æº–ç¢ºç‡'].mean(), color='red', linestyle='--', linewidth=2,
                     label=f'å¹³å‡å€¼: {cv_df["æº–ç¢ºç‡"].mean():.3f}')
        ax_cv.set_ylabel('æº–ç¢ºç‡', fontsize=14)
        ax_cv.set_xlabel('äº¤å‰é©—è­‰æŠ˜æ•¸', fontsize=14)
        ax_cv.set_title('5-Fold äº¤å‰é©—è­‰çµæœ', fontsize=16, fontweight='bold')
        ax_cv.legend(fontsize=12)
        ax_cv.grid(True, alpha=0.3)
        ax_cv.set_ylim(0, 1.1)
        
        # åœ¨æŸ±ç‹€åœ–ä¸Šé¡¯ç¤ºæ•¸å€¼
        for bar, score in zip(bars, results['cv_scores']):
            height = bar.get_height()
            ax_cv.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                      f'{score:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')
        
        st.pyplot(fig_cv)
        
        # æ·»åŠ ä¸‹è¼‰æŒ‰éˆ•
        buffer = create_downloadable_plot(fig_cv, "cross_validation_results.png")
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰äº¤å‰é©—è­‰åœ–è¡¨",
            data=buffer,
            file_name="cross_validation_results.png",
            mime="image/png"
        )
        plt.close(fig_cv)
        st.markdown('</div>', unsafe_allow_html=True)
        
        # === å„é¡åˆ¥æ€§èƒ½æŒ‡æ¨™å’Œä¿¡å¿ƒåº¦çµ±è¨ˆä¸¦åˆ— ===
        st.markdown('<div class="card-container">', unsafe_allow_html=True)
        st.subheader("ğŸ“Š æ€§èƒ½æŒ‡æ¨™è©³æƒ…")
        
        # ä½¿ç”¨å…©åˆ—ä½ˆå±€
        perf_col1, perf_col2 = st.columns(2)
        
        with perf_col1:
            st.write("**ğŸ“Š å„é¡åˆ¥æ€§èƒ½æŒ‡æ¨™**")
            # å‰µå»ºè©³ç´°çš„æ€§èƒ½æŒ‡æ¨™è¡¨
            performance_df = pd.DataFrame({
                'é¡åˆ¥': target_names,
                'ç²¾ç¢ºç‡': [f"{p:.3f}" for p in results['precision']],
                'å¬å›ç‡': [f"{r:.3f}" for r in results['recall']],
                'F1-Score': [f"{f:.3f}" for f in results['f1']],
                'æ¨£æœ¬æ•¸': results['support'].astype(int)
            })
            st.dataframe(performance_df, use_container_width=True)
        
        with perf_col2:
            st.write("**ğŸ“Š é æ¸¬ä¿¡å¿ƒåº¦çµ±è¨ˆ**")
            # ä¿¡å¿ƒåº¦çµ±è¨ˆ
            max_probas = np.max(results['y_pred_proba'], axis=1)
            confidence_stats = pd.DataFrame({
                'æŒ‡æ¨™': ['å¹³å‡ä¿¡å¿ƒåº¦', 'æœ€ä½ä¿¡å¿ƒåº¦', 'æœ€é«˜ä¿¡å¿ƒåº¦', 'æ¨™æº–å·®'],
                'æ•¸å€¼': [f"{max_probas.mean():.3f}", f"{max_probas.min():.3f}", 
                        f"{max_probas.max():.3f}", f"{max_probas.std():.3f}"]
            })
            st.dataframe(confidence_stats, use_container_width=True)
        
        # === æ€§èƒ½æŒ‡æ¨™é›·é”åœ– ===
        st.write("**ğŸ“Š å„é¡åˆ¥æ€§èƒ½æŒ‡æ¨™é›·é”åœ–**")
        categories = target_names
        fig_radar, ax_radar = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))
        
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False)
        angles = np.concatenate((angles, [angles[0]]))
        
        # ç¹ªè£½ç²¾ç¢ºç‡ã€å¬å›ç‡ã€F1-Score
        for metric_name, metric_values, color in [
            ('ç²¾ç¢ºç‡', results['precision'], 'blue'),
            ('å¬å›ç‡', results['recall'], 'green'),
            ('F1-Score', results['f1'], 'red')
        ]:
            values = np.concatenate((metric_values, [metric_values[0]]))
            ax_radar.plot(angles, values, 'o-', linewidth=2, label=metric_name, color=color)
            ax_radar.fill(angles, values, alpha=0.25, color=color)
        
        ax_radar.set_xticks(angles[:-1])
        ax_radar.set_xticklabels(categories, fontsize=12)
        ax_radar.set_ylim(0, 1)
        ax_radar.set_title('å„é¡åˆ¥æ€§èƒ½æŒ‡æ¨™é›·é”åœ–', pad=20, fontsize=16)
        ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax_radar.grid(True)
        
        st.pyplot(fig_radar)
        
        buffer = create_downloadable_plot(fig_radar, "performance_radar_chart.png")
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰é›·é”åœ–",
            data=buffer,
            file_name="performance_radar_chart.png",
            mime="image/png"
        )
        plt.close(fig_radar)
        st.markdown('</div>', unsafe_allow_html=True)
        
        # === è¦–è¦ºåŒ–åˆ†æ ===
        st.markdown('<div class="card-container">', unsafe_allow_html=True)
        st.subheader("ğŸ“Š è¦–è¦ºåŒ–åˆ†æ")
        
        visual_col1, visual_col2 = st.columns(2)
        
        with visual_col1:
            st.write("**ğŸ”¥ æ··æ·†çŸ©é™£**")
            cm = confusion_matrix(y_test, results['y_pred_test'])
            fig_cm, ax_cm = plt.subplots(figsize=(6, 5))
            
            # è¨ˆç®—ç™¾åˆ†æ¯”
            cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            
            # å‰µå»ºæ¨™è¨»
            annot = []
            for i in range(cm.shape[0]):
                row = []
                for j in range(cm.shape[1]):
                    row.append(f'{cm[i,j]}\n({cm_percent[i,j]:.1%})')
                annot.append(row)
            
            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=target_names, yticklabels=target_names, ax=ax_cm)
            ax_cm.set_xlabel('é æ¸¬æ¨™ç±¤')
            ax_cm.set_ylabel('çœŸå¯¦æ¨™ç±¤')
            ax_cm.set_title('æ··æ·†çŸ©é™£ (æ•¸é‡ & ç™¾åˆ†æ¯”)')
            st.pyplot(fig_cm)
            
            # æ·»åŠ ä¸‹è¼‰æŒ‰éˆ•
            buffer = create_downloadable_plot(fig_cm, "confusion_matrix.png")
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰æ··æ·†çŸ©é™£",
                data=buffer,
                file_name="confusion_matrix.png",
                mime="image/png"
            )
            plt.close(fig_cm)
            
        with visual_col2:
            st.write("**ğŸ¯ é æ¸¬ä¿¡å¿ƒåº¦åˆ†å¸ƒ**")
            # åˆ†æé æ¸¬ä¿¡å¿ƒåº¦åˆ†å¸ƒ
            max_probas = np.max(results['y_pred_proba'], axis=1)
            
            fig_conf, ax_conf = plt.subplots(figsize=(6, 5))
            n, bins, patches = ax_conf.hist(max_probas, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
            ax_conf.axvline(x=max_probas.mean(), color='red', linestyle='--', 
                           label=f'å¹³å‡ä¿¡å¿ƒåº¦: {max_probas.mean():.3f}')
            ax_conf.set_xlabel('æœ€å¤§é æ¸¬æ©Ÿç‡')
            ax_conf.set_ylabel('æ¨£æœ¬æ•¸')
            ax_conf.set_title('æ¨¡å‹é æ¸¬ä¿¡å¿ƒåº¦åˆ†å¸ƒ')
            ax_conf.legend()
            ax_conf.grid(True, alpha=0.3)
            
            st.pyplot(fig_conf)
            
            buffer = create_downloadable_plot(fig_conf, "confidence_distribution.png")
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰ä¿¡å¿ƒåº¦åˆ†å¸ƒåœ–",
                data=buffer,
                file_name="confidence_distribution.png",
                mime="image/png"
            )
            plt.close(fig_conf)
        st.markdown('</div>', unsafe_allow_html=True)
        
        # === å­¸ç¿’æ›²ç·šåˆ†æ ===
        if hasattr(results['mlp'], 'loss_curve_') and results['mlp'].loss_curve_:
            st.markdown('<div class="card-container">', unsafe_allow_html=True)
            st.subheader("ğŸ“‰ å­¸ç¿’æ›²ç·šåˆ†æ")
            
            st.write("**ğŸ”» è¨“ç·´æå¤±æ›²ç·š**")
            fig_loss, ax_loss = plt.subplots(figsize=(10, 5))
            ax_loss.plot(results['mlp'].loss_curve_, 'b-', linewidth=2, label='è¨“ç·´æå¤±')
            ax_loss.set_xlabel('è¿­ä»£æ¬¡æ•¸')
            ax_loss.set_ylabel('æå¤±å€¼')
            ax_loss.set_title('è¨“ç·´éç¨‹æå¤±è®ŠåŒ–')
            ax_loss.grid(True, alpha=0.3)
            ax_loss.legend()
            
            # æ¨™è¨»é‡è¦é»
            min_loss_idx = np.argmin(results['mlp'].loss_curve_)
            min_loss_val = results['mlp'].loss_curve_[min_loss_idx]
            ax_loss.plot(min_loss_idx, min_loss_val, 'ro', markersize=8, 
                       label=f'æœ€ä½æå¤±: {min_loss_val:.4f}')
            ax_loss.legend()
            
            st.pyplot(fig_loss)
            
            buffer = create_downloadable_plot(fig_loss, "learning_curve.png")
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰å­¸ç¿’æ›²ç·š",
                data=buffer,
                file_name="learning_curve.png",
                mime="image/png"
            )
            plt.close(fig_loss)
            
            # å­¸ç¿’çµ±è¨ˆ
            st.write("**ğŸ“Š å­¸ç¿’çµ±è¨ˆ**")
            loss_curve = results['mlp'].loss_curve_
            learning_stats = pd.DataFrame({
                'æŒ‡æ¨™': ['æœ€çµ‚æå¤±', 'æœ€ä½æå¤±', 'åˆå§‹æå¤±', 'æå¤±ä¸‹é™ç‡'],
                'æ•¸å€¼': [
                    f"{loss_curve[-1]:.4f}",
                    f"{np.min(loss_curve):.4f}",
                    f"{loss_curve[0]:.4f}",
                    f"{((loss_curve[0] - loss_curve[-1]) / loss_curve[0] * 100):.1f}%"
                ]
            })
            st.dataframe(learning_stats, use_container_width=True)
            st.markdown('</div>', unsafe_allow_html=True)
        
        # === æ±ºç­–é‚Šç•Œè¦–è¦ºåŒ– ===
        st.markdown('<div class="card-container">', unsafe_allow_html=True)
        st.subheader("ğŸ¨ æ±ºç­–é‚Šç•Œè¦–è¦ºåŒ–")
        
        if len(selected_features) >= 2:
            st.write("**é¸æ“‡å…©å€‹ç‰¹å¾µä¾†ç¹ªè£½æ±ºç­–é‚Šç•Œï¼š**")
            
            boundary_col1, boundary_col2 = st.columns(2)
            with boundary_col1:
                feature_1 = st.selectbox(
                    "é¸æ“‡ç¬¬ä¸€å€‹ç‰¹å¾µï¼ˆXè»¸ï¼‰",
                    options=selected_features,
                    index=0,
                    key="boundary_feature_1"
                )
            with boundary_col2:
                feature_2 = st.selectbox(
                    "é¸æ“‡ç¬¬äºŒå€‹ç‰¹å¾µï¼ˆYè»¸ï¼‰",
                    options=[f for f in selected_features if f != feature_1],
                    index=0,
                    key="boundary_feature_2"
                )
            
            if st.button("ğŸ¨ ç¹ªè£½æ±ºç­–é‚Šç•Œ", type="secondary"):
                with st.spinner("æ­£åœ¨ç¹ªè£½æ±ºç­–é‚Šç•Œ..."):
                    # ç²å–ç‰¹å¾µç´¢å¼•
                    feature_indices = [selected_features.index(feature_1), 
                                     selected_features.index(feature_2)]
                    feature_names_selected = [feature_1, feature_2]
                    
                    # ç¹ªè£½æ±ºç­–é‚Šç•Œ
                    fig_boundary = plot_decision_boundary(
                        results['mlp'], 
                        X_train, 
                        y_train, 
                        feature_indices,
                        feature_names_selected,
                        target_names,
                        loaded_scaler
                    )
                    
                    st.pyplot(fig_boundary)
                    
                    # æ·»åŠ ä¸‹è¼‰æŒ‰éˆ•
                    buffer = create_downloadable_plot(fig_boundary, "decision_boundary.png")
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰æ±ºç­–é‚Šç•Œåœ–",
                        data=buffer,
                        file_name="decision_boundary.png",
                        mime="image/png"
                    )
                    plt.close(fig_boundary)
                    
                    # æ·»åŠ èªªæ˜
                    st.info("""
                    **åœ–è¡¨èªªæ˜ï¼š**
                    - èƒŒæ™¯é¡è‰²ä»£è¡¨æ¨¡å‹çš„æ±ºç­–å€åŸŸ
                    - æ•£é»ä»£è¡¨è¨“ç·´æ•¸æ“š
                    - ä¸åŒé¡è‰²ä»£è¡¨ä¸åŒçš„é³¶å°¾èŠ±ç¨®é¡
                    - é‚Šç•Œç·šé¡¯ç¤ºäº†æ¨¡å‹å¦‚ä½•å€åˆ†ä¸åŒé¡åˆ¥
                    """)
        else:
            st.warning("âš ï¸ éœ€è¦è‡³å°‘é¸æ“‡2å€‹ç‰¹å¾µæ‰èƒ½ç¹ªè£½æ±ºç­–é‚Šç•Œ")
        st.markdown('</div>', unsafe_allow_html=True)
        
        # === æ¨¡å‹è©³ç´°è³‡è¨Š ===
        st.markdown('<div class="card-container">', unsafe_allow_html=True)
        st.subheader("ğŸ” æ¨¡å‹è©³ç´°è³‡è¨Š")
        
        model_info_col1, model_info_col2 = st.columns(2)
        
        with model_info_col1:
            st.write("**ğŸ—ï¸ æ¨¡å‹æ¶æ§‹**")
            architecture_info = pd.DataFrame({
                'å±¤ç´š': ['è¼¸å…¥å±¤'] + [f'éš±è—å±¤{i+1}' for i in range(len(results['mlp'].coefs_)-1)] + ['è¼¸å‡ºå±¤'],
                'ç¥ç¶“å…ƒæ•¸': [results['mlp'].coefs_[0].shape[0]] + 
                          [coef.shape[1] for coef in results['mlp'].coefs_[:-1]] + 
                          [results['mlp'].coefs_[-1].shape[1]]
            })
            st.dataframe(architecture_info, use_container_width=True)
        
        with model_info_col2:
            st.write("**ğŸ“Š è¨“ç·´è³‡è¨Š**")
            convergence = results['convergence_info']
            # å°‡æ‰€æœ‰æ•¸å€¼è½‰æ›ç‚ºå­—ä¸²ä»¥é¿å…æ··åˆé¡å‹éŒ¯èª¤
            training_info = pd.DataFrame({
                'æŒ‡æ¨™': ['å¯¦éš›è¿­ä»£æ¬¡æ•¸', 'æœ€å¤§è¿­ä»£æ¬¡æ•¸', 'æ”¶æ–‚ç‹€æ…‹', 'æ¬Šé‡åƒæ•¸ç¸½æ•¸', 'åç½®åƒæ•¸ç¸½æ•¸', 'ç¸½åƒæ•¸é‡'],
                'æ•¸å€¼': [
                    str(convergence['actual_iterations']),
                    str(convergence['max_iterations']),
                    "âœ… å·²æ”¶æ–‚" if convergence['converged'] else "âŒ æœªæ”¶æ–‚",
                    str(sum(coef.size for coef in results['mlp'].coefs_)),
                    str(sum(intercept.size for intercept in results['mlp'].intercepts_)),
                    str(sum(coef.size for coef in results['mlp'].coefs_) + 
                        sum(intercept.size for intercept in results['mlp'].intercepts_))
                ]
            })
            st.dataframe(training_info, use_container_width=True)
        st.markdown('</div>', unsafe_allow_html=True)
        
        # === å¿«é€Ÿæ“ä½œ ===
        st.markdown('<div class="card-container">', unsafe_allow_html=True)
        st.subheader("âš¡ å¿«é€Ÿæ“ä½œ")
        
        op_col1, op_col2, op_col3 = st.columns(3)
        
        with op_col1:
            if st.button("ğŸ”„ é‡æ–°è¨“ç·´", type="secondary", use_container_width=True):
                st.info("ğŸ’¡ è«‹åˆ‡æ›åˆ°ã€ŒğŸ¯ æ¨¡å‹è¨“ç·´ã€æ¨™ç±¤é ")
        
        with op_col2:
            if st.button("ğŸ”® å‰å¾€é æ¸¬", type="secondary", use_container_width=True):
                st.info("ğŸ’¡ è«‹åˆ‡æ›åˆ°ã€ŒğŸ”® å³æ™‚é æ¸¬ã€æ¨™ç±¤é ")
        
        with op_col3:
            # å‰µå»ºç°¡å–®çš„å ±å‘Šæ‘˜è¦
            report_data = {
                "æ¸¬è©¦æº–ç¢ºç‡": results['test_accuracy'],
                "äº¤å‰é©—è­‰å‡å€¼": results['cv_scores'].mean(),
                "äº¤å‰é©—è­‰æ¨™æº–å·®": results['cv_scores'].std(),
                "F1-Score": results['f1'].mean(),
                "éæ“¬åˆç¨‹åº¦": results['train_accuracy'] - results['test_accuracy']
            }
            
            report_df = pd.DataFrame(list(report_data.items()), columns=['æŒ‡æ¨™', 'æ•¸å€¼'])
            csv = report_df.to_csv(index=False)
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰å ±å‘Š",
                data=csv,
                file_name="mlp_training_report.csv",
                mime="text/csv",
                use_container_width=True
            )
        st.markdown('</div>', unsafe_allow_html=True)
    
    elif model_file_exists:
        # å¦‚æœæ¨¡å‹æ–‡ä»¶å­˜åœ¨ä½†session stateä¸ä¸€è‡´ï¼Œå˜—è©¦è¼‰å…¥
        st.info("ç™¼ç¾æ¨¡å‹æ–‡ä»¶ï¼Œæ­£åœ¨è¼‰å…¥...")
        try:
            loaded_mlp_model = joblib.load(MODEL_PATH)
            loaded_data_scaler = joblib.load(SCALER_PATH)
            
            # åŸ·è¡Œè©•ä¼°
            evaluation_results = comprehensive_evaluation(
                loaded_mlp_model, X_train, X_test, y_train, y_test, target_names
            )
            
            # æ›´æ–°session state
            st.session_state.training_results = {
                'mlp': loaded_mlp_model,
                'selected_features': selected_features,
                **evaluation_results
            }
            st.session_state.model_trained = True
             #æç¤ºç”¨æˆ¶åˆ·æ–°é é¢
            st.success("âœ… æ¨¡å‹å·²æˆåŠŸè¼‰å…¥! è«‹æ‰‹å‹•åˆ·æ–°é é¢ä»¥æŸ¥çœ‹çµæœ")
            st.button("åˆ·æ–°é é¢", on_click=lambda: None)  # é€™å€‹æŒ‰éˆ•åªæ˜¯æç¤ºç”¨æˆ¶åˆ·æ–°é é¢
        except Exception as e:
            st.error(f"è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
    else:
        st.warning("âš ï¸ è«‹å…ˆåœ¨ã€ŒğŸ¯ æ¨¡å‹è¨“ç·´ã€æ¨™ç±¤é è¨“ç·´æ¨¡å‹")

# --- Tab 3: å³æ™‚é æ¸¬ ---
with tabs[2]:
    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
    if model_exists:
        try:
            loaded_mlp_model = joblib.load(MODEL_PATH)
            loaded_data_scaler = joblib.load(SCALER_PATH)
            
            st.success("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼Œå¯ä»¥é€²è¡Œé æ¸¬ï¼")
            
            # é æ¸¬ç•Œé¢
            st.markdown('<div class="card-container">', unsafe_allow_html=True)
            st.subheader("ğŸ”® è¼¸å…¥ç‰¹å¾µå€¼é€²è¡Œé æ¸¬")
            
            # ä½¿ç”¨åŸå§‹è³‡æ–™ä½œç‚ºåƒè€ƒ
            if 'original_X_df' in st.session_state and 'original_y' in st.session_state:
                original_X_df = st.session_state.original_X_df
                original_y = st.session_state.original_y
                
                st.write("**å¾åŸå§‹é³¶å°¾èŠ±è³‡æ–™é›†è¼‰å…¥æ¨£æœ¬ä½œç‚ºèµ·é»ï¼š**")
                sample_id_to_load = st.selectbox(
                    f"é¸æ“‡ä¸€å€‹åŸå§‹æ¨£æœ¬ ID (0 åˆ° {len(original_X_df) - 1}):",
                    options=list(range(len(original_X_df))),
                    index=0,
                    key="sample_id_selector_tab3"
                )
                
                initially_loaded_sample_features = original_X_df.iloc[sample_id_to_load]
                initially_loaded_true_label_index = original_y[sample_id_to_load]
                initially_loaded_true_label_name = target_names[initially_loaded_true_label_index]
                
                st.info(f"ğŸ“Œ ç•¶å‰é¸æ“‡æ¨£æœ¬ ID: {sample_id_to_load}ï¼Œå…¶çœŸå¯¦é¡åˆ¥ç‚º: **{initially_loaded_true_label_name}**")
                st.caption("ğŸ’¡ ä¸‹æ–¹çš„ç‰¹å¾µå€¼å·²å¾æ‰€é¸æ¨£æœ¬è‡ªå‹•å¡«å…¥ï¼Œæ‚¨å¯ä»¥è‡ªç”±èª¿æ•´å®ƒå€‘ã€‚")
                
                # å‰µå»ºè¼¸å…¥è¡¨å–®
                with st.form("prediction_form"):
                    st.write("**è«‹è¼¸å…¥æˆ–èª¿æ•´ç‰¹å¾µå€¼ï¼š**")
                    
                    input_data = {}
                    cols = st.columns(2)  # å‰µå»ºå…©åˆ—ä»¥æ”¹å–„å¸ƒå±€
                    
                    for i, feature in enumerate(all_feature_names):
                        col_idx = i % 2
                        with cols[col_idx]:
                            # ç²å–é è¨­å€¼ä¸¦é™åˆ¶ç‚º1ä½å°æ•¸
                            default_value = round(float(initially_loaded_sample_features.get(feature, 0.0)), 1)
                            
                            # è¨ˆç®—åˆç†çš„ç¯„åœ
                            feature_values = original_X_df[feature]
                            min_val = float(feature_values.min())
                            max_val = float(feature_values.max())
                            mean_val = float(feature_values.mean())
                            
                            input_data[feature] = st.number_input(
                                f'ğŸ“ {feature}',
                                value=default_value,
                                min_value=round(min_val - 1.0, 1),
                                max_value=round(max_val + 1.0, 1),
                                step=0.1,
                                format="%.1f",  # é™åˆ¶é¡¯ç¤ºæ ¼å¼ç‚º1ä½å°æ•¸
                                help=f"åŸå§‹è³‡æ–™ç¯„åœ: {min_val:.1f} ~ {max_val:.1f}ï¼Œå¹³å‡å€¼: {mean_val:.1f}",
                                key=f"input_{feature}_tab3"
                            )
                    
                    predict_button = st.form_submit_button("ğŸ”® é–‹å§‹é æ¸¬", type="primary", use_container_width=True)
                
                if predict_button:
                    try:
                        # æº–å‚™è¼¸å…¥æ•¸æ“š
                        final_input_features_dict = input_data.copy()
                        final_input_features_array = np.array([final_input_features_dict[f] for f in all_feature_names])
                        
                        # æŸ¥æ‰¾æ˜¯å¦åŒ¹é…åŸå§‹è³‡æ–™ä¸­çš„æ¨£æœ¬
                        label_for_comparison = initially_loaded_true_label_name
                        source_of_label_info = f"æ¨£æœ¬ ID {sample_id_to_load}"
                        found_match = False
                        matched_sample_id = sample_id_to_load
                        
                        # ä½¿ç”¨å¯¬é¬†çš„å®¹å·®é€²è¡Œæ¯”å°
                        for idx, original_row_values in enumerate(original_X_df.values):
                            # å°‡åŸå§‹è³‡æ–™å’Œè¼¸å…¥éƒ½å››æ¨äº”å…¥åˆ°1ä½å°æ•¸é€²è¡Œæ¯”è¼ƒ
                            rounded_original = np.round(original_row_values, 1)
                            rounded_input = np.round(final_input_features_array, 1)
                            
                            if np.allclose(rounded_input, rounded_original, atol=1e-2, rtol=0):
                                matched_label = target_names[original_y[idx]]
                                label_for_comparison = matched_label
                                matched_sample_id = idx
                                source_of_label_info = f"æ¨£æœ¬ ID {idx}"
                                
                                if idx != sample_id_to_load:
                                    st.success(f"ğŸ” ç™¼ç¾åŒ¹é…ï¼æ‚¨è¼¸å…¥çš„ç‰¹å¾µå€¼èˆ‡æ¨£æœ¬ ID {idx} ({matched_label}) å®Œå…¨ä¸€è‡´ã€‚")
                                    found_match = True
                                    break
                        
                        if not found_match and matched_sample_id != sample_id_to_load:
                            st.info("ğŸ’¡ ç•¶å‰è¼¸å…¥ç‚ºè‡ªå®šç¾©ç‰¹å¾µçµ„åˆã€‚")
                        
                        # è™•ç†åªä½¿ç”¨é¸å®šç‰¹å¾µçš„æƒ…æ³
                        full_input_df = pd.DataFrame([final_input_features_dict])[all_feature_names]
                        input_scaled = loaded_data_scaler.transform(full_input_df)
                        
                        # åªé¸æ“‡æ¨¡å‹è¨“ç·´æ™‚ä½¿ç”¨çš„ç‰¹å¾µ
                        selected_indices = [all_feature_names.index(f) for f in selected_features]
                        input_for_prediction = input_scaled[:, selected_indices]
                        
                        # æ¨¡å‹é æ¸¬
                        prediction_proba = loaded_mlp_model.predict_proba(input_for_prediction)
                        prediction_class = np.argmax(prediction_proba)
                        predicted_label = target_names[prediction_class]
                        confidence = prediction_proba[0][prediction_class]
                        
                        # === é¡¯ç¤ºé æ¸¬çµæœ ===
                        st.markdown("---")
                        st.subheader("ğŸ‰ é æ¸¬çµæœ")
                        
                        # ä¸»è¦çµæœå±•ç¤º
                        result_col1, result_col2, result_col3 = st.columns([2, 2, 1])
                        
                        with result_col1:
                            st.metric(
                                "ğŸŒ¸ é æ¸¬é¡åˆ¥",
                                predicted_label,
                                delta=f"ä¿¡å¿ƒåº¦: {confidence:.1%}"
                            )
                        
                        with result_col2:
                            st.metric(
                                "ğŸ“Š åƒè€ƒç­”æ¡ˆ",
                                label_for_comparison,
                                delta=source_of_label_info
                            )
                        
                        with result_col3:
                            if predicted_label == label_for_comparison:
                                st.success("âœ… æ­£ç¢º")
                            else:
                                st.error("âŒ éŒ¯èª¤")
                        
                        # æ©Ÿç‡åˆ†å¸ƒè¦–è¦ºåŒ–
                        st.markdown("---")
                        st.subheader("ğŸ“Š é æ¸¬æ©Ÿç‡åˆ†å¸ƒ")
                        
                        # å‰µå»ºæ©Ÿç‡DataFrame
                        proba_df = pd.DataFrame({
                            'ç¨®é¡': target_names,
                            'æ©Ÿç‡': prediction_proba[0]
                        }).sort_values('æ©Ÿç‡', ascending=True)
                        
                        # ç¹ªè£½æ©«æ¢åœ–
                        fig_pred, ax_pred = plt.subplots(figsize=(10, 6))
                        
                        # ä½¿ç”¨ä¸åŒé¡è‰²æ¨™è¨˜é æ¸¬é¡åˆ¥
                        colors = ['#ff7f7f' if name == predicted_label else '#87ceeb' 
                                 for name in proba_df['ç¨®é¡']]
                        
                        bars = ax_pred.barh(proba_df['ç¨®é¡'], proba_df['æ©Ÿç‡'], color=colors, alpha=0.8)
                        
                        # è¨­ç½®åœ–è¡¨å±¬æ€§
                        ax_pred.set_xlabel('é æ¸¬æ©Ÿç‡', fontsize=12)
                        ax_pred.set_title('å„é¡åˆ¥é æ¸¬æ©Ÿç‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
                        ax_pred.set_xlim(0, 1)
                        ax_pred.grid(True, alpha=0.3, axis='x')
                        
                        # åœ¨æ©«æ¢ä¸Šé¡¯ç¤ºæ•¸å€¼
                        for i, (bar, prob) in enumerate(zip(bars, proba_df['æ©Ÿç‡'])):
                            width = bar.get_width()
                            if width > 0.1:  # åªåœ¨æ©Ÿç‡å¤§æ–¼10%æ™‚åœ¨æ¢å…§é¡¯ç¤º
                                ax_pred.text(width/2, bar.get_y() + bar.get_height()/2, 
                                           f'{width:.1%}', ha='center', va='center', 
                                           fontweight='bold', color='white')
                            else:  # å¦å‰‡åœ¨æ¢å¤–é¡¯ç¤º
                                ax_pred.text(width + 0.01, bar.get_y() + bar.get_height()/2, 
                                           f'{width:.1%}', ha='left', va='center', 
                                           fontweight='bold')
                        
                        # æ·»åŠ é æ¸¬æ¨™è¨˜
                        for i, name in enumerate(proba_df['ç¨®é¡']):
                            if name == predicted_label:
                                ax_pred.text(1.02, i, 'â† é æ¸¬', va='center', fontweight='bold', color='red')
                        
                        plt.tight_layout()
                        st.pyplot(fig_pred)
                        
                        # æ·»åŠ ä¸‹è¼‰æŒ‰éˆ•
                        buffer = create_downloadable_plot(fig_pred, "prediction_probability.png")
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è¼‰é æ¸¬æ©Ÿç‡åœ–",
                            data=buffer,
                            file_name="prediction_probability.png",
                            mime="image/png",
                            key="download_pred_prob"
                        )
                        plt.close(fig_pred)
                        
                        # è©³ç´°æ©Ÿç‡è¡¨æ ¼
                        st.subheader("ğŸ“‹ è©³ç´°é æ¸¬è³‡è¨Š")
                        
                        # å‰µå»ºè©³ç´°è³‡è¨Šè¡¨
                        detailed_proba = pd.DataFrame({
                            'èŠ±çš„ç¨®é¡': target_names,
                            'é æ¸¬æ©Ÿç‡': [f"{p:.2%}" for p in prediction_proba[0]],
                            'ä¿¡å¿ƒç­‰ç´š': ['ğŸ”¥ é«˜' if p > 0.7 else 'âš¡ ä¸­' if p > 0.4 else 'ğŸ’¤ ä½' 
                                        for p in prediction_proba[0]],
                            'æ’å': [f"ç¬¬ {i+1} å" for i in range(len(target_names))]
                        })
                        
                        # æŒ‰æ©Ÿç‡æ’åº
                        detailed_proba = detailed_proba.sort_values('é æ¸¬æ©Ÿç‡', ascending=False)
                        detailed_proba.index = range(1, len(detailed_proba) + 1)
                        
                        st.dataframe(detailed_proba, use_container_width=True)
                        
                        # ç‰¹å¾µè²¢ç»åˆ†æï¼ˆå¦‚æœåªé¸äº†éƒ¨åˆ†ç‰¹å¾µï¼‰
                        if len(selected_features) < len(all_feature_names):
                            st.info(f"ğŸ’¡ **æ³¨æ„**ï¼šæ¨¡å‹é æ¸¬åƒ…åŸºæ–¼ {len(selected_features)} å€‹é¸å®šç‰¹å¾µ: {', '.join(selected_features)}")
                        
                        # é æ¸¬è§£é‡‹
                        st.subheader("ğŸ” é æ¸¬è§£é‡‹")
                        
                        if confidence > 0.9:
                            st.success(f"æ¨¡å‹å°é æ¸¬çµæœ **{predicted_label}** éå¸¸æœ‰ä¿¡å¿ƒï¼ˆ{confidence:.1%}ï¼‰ï¼")
                        elif confidence > 0.7:
                            st.info(f"æ¨¡å‹è¼ƒæœ‰ä¿¡å¿ƒé æ¸¬ç‚º **{predicted_label}**ï¼ˆ{confidence:.1%}ï¼‰ã€‚")
                        else:
                            st.warning(f"æ¨¡å‹é æ¸¬ç‚º **{predicted_label}**ï¼Œä½†ä¿¡å¿ƒåº¦è¼ƒä½ï¼ˆ{confidence:.1%}ï¼‰ï¼Œå»ºè­°è¬¹æ…åƒè€ƒã€‚")
                        
                        # è¼¸å…¥ç‰¹å¾µæ‘˜è¦
                        with st.expander("ğŸ“ æŸ¥çœ‹è¼¸å…¥ç‰¹å¾µæ‘˜è¦"):
                            input_summary = pd.DataFrame({
                                'ç‰¹å¾µåç¨±': all_feature_names,
                                'è¼¸å…¥å€¼': [f"{input_data[f]:.1f}" for f in all_feature_names],
                                'æ˜¯å¦ç”¨æ–¼é æ¸¬': ['âœ… æ˜¯' if f in selected_features else 'âŒ å¦' for f in all_feature_names]
                            })
                            st.dataframe(input_summary, use_container_width=True)
                        
                    except Exception as e:
                        st.error(f"âŒ é æ¸¬éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                        import traceback
                        st.text("è©³ç´°éŒ¯èª¤ä¿¡æ¯ï¼š")
                        st.code(traceback.format_exc())
                
                # å¿«é€Ÿæ¸¬è©¦æŒ‰éˆ•
                st.markdown("---")
                st.subheader("âš¡ å¿«é€Ÿæ¸¬è©¦")
                
                quick_test_col1, quick_test_col2, quick_test_col3 = st.columns(3)
                
                with quick_test_col1:
                    if st.button("ğŸŒº æ¸¬è©¦ Setosa æ¨£æœ¬", use_container_width=True):
                        st.info("è«‹é¸æ“‡æ¨£æœ¬ ID 0-49 ä¸­çš„ä»»ä¸€å€‹")
                
                with quick_test_col2:
                    if st.button("ğŸŒ¸ æ¸¬è©¦ Versicolor æ¨£æœ¬", use_container_width=True):
                        st.info("è«‹é¸æ“‡æ¨£æœ¬ ID 50-99 ä¸­çš„ä»»ä¸€å€‹")
                
                with quick_test_col3:
                    if st.button("ğŸŒ¼ æ¸¬è©¦ Virginica æ¨£æœ¬", use_container_width=True):
                        st.info("è«‹é¸æ“‡æ¨£æœ¬ ID 100-149 ä¸­çš„ä»»ä¸€å€‹")
                
            else:
                # å¦‚æœæ²’æœ‰åŸå§‹è³‡æ–™ï¼Œæä¾›æ‰‹å‹•è¼¸å…¥
                st.warning("âš ï¸ ç„¡æ³•è¼‰å…¥åŸå§‹è³‡æ–™é›†ï¼Œè«‹æ‰‹å‹•è¼¸å…¥ç‰¹å¾µå€¼ã€‚")
                
                with st.form("manual_prediction_form"):
                    st.write("**è«‹è¼¸å…¥ç‰¹å¾µå€¼ï¼š**")
                    
                    input_data = {}
                    for feature in selected_features:
                        input_data[feature] = st.number_input(
                            f'ğŸ“ {feature}',
                            value=0.0,
                            step=0.1,
                            format="%.1f",
                            key=f"manual_input_{feature}"
                        )
                    
                    predict_button = st.form_submit_button("ğŸ”® é–‹å§‹é æ¸¬", type="primary", use_container_width=True)
                
                if predict_button:
                    st.info("è«‹ç¢ºä¿æ‚¨çš„è¼¸å…¥å€¼å·²ç¶“éé©ç•¶çš„æ¨™æº–åŒ–è™•ç†ã€‚")
            st.markdown('</div>', unsafe_allow_html=True)
        
        except Exception as e:
            st.error(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼š{e}")
            st.text("è«‹ç¢ºä¿æ¨¡å‹æª”æ¡ˆå­˜åœ¨ä¸”æœªæå£ã€‚")
    
    else:
        st.warning("âš ï¸ è«‹å…ˆåœ¨ã€ŒğŸ¯ æ¨¡å‹è¨“ç·´ã€æ¨™ç±¤é è¨“ç·´æ¨¡å‹")
        st.info("ğŸ’¡ è¨“ç·´å®Œæˆå¾Œå³å¯åœ¨æ­¤é€²è¡Œå³æ™‚é æ¸¬")
        
        # æä¾›ç¯„ä¾‹èªªæ˜
        with st.expander("ğŸ“– ä½¿ç”¨èªªæ˜"):
            st.markdown("""
            **å¦‚ä½•ä½¿ç”¨å³æ™‚é æ¸¬åŠŸèƒ½ï¼š**
            
            1. **è¨“ç·´æ¨¡å‹**ï¼šå…ˆåœ¨ã€Œæ¨¡å‹è¨“ç·´ã€é é¢å®Œæˆæ¨¡å‹è¨“ç·´
            2. **é¸æ“‡æ¨£æœ¬**ï¼šå¾ä¸‹æ‹‰é¸å–®é¸æ“‡ä¸€å€‹åŸå§‹æ¨£æœ¬ä½œç‚ºèµ·é»
            3. **èª¿æ•´ç‰¹å¾µ**ï¼šæ ¹æ“šéœ€è¦èª¿æ•´å„å€‹ç‰¹å¾µå€¼
            4. **é€²è¡Œé æ¸¬**ï¼šé»æ“Šé æ¸¬æŒ‰éˆ•æŸ¥çœ‹çµæœ
            5. **åˆ†æçµæœ**ï¼šæŸ¥çœ‹é æ¸¬é¡åˆ¥ã€ä¿¡å¿ƒåº¦å’Œæ©Ÿç‡åˆ†å¸ƒ
            
            **æç¤º**ï¼š
            - ç‰¹å¾µå€¼æœƒè‡ªå‹•é™åˆ¶ç‚º1ä½å°æ•¸ï¼Œç¢ºä¿è¼¸å…¥ç²¾åº¦ä¸€è‡´
            - ç³»çµ±æœƒè‡ªå‹•æª¢æ¸¬æ‚¨çš„è¼¸å…¥æ˜¯å¦åŒ¹é…åŸå§‹è³‡æ–™é›†ä¸­çš„æ¨£æœ¬
            - å¯ä»¥ä¸‹è¼‰é æ¸¬çµæœåœ–è¡¨ç”¨æ–¼å ±å‘Šæˆ–åˆ†äº«
            """)

# --- é è…³ ---
st.markdown("---")
st.subheader("ğŸ“š ç›¸é—œè³‡æº")

st.markdown("**ğŸ“Š è³‡æ–™ä¾†æºï¼š** [Scikit-learn Iris Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)")
st.markdown("**âš¡ éƒ¨ç½²å¹³å°ï¼š** [Streamlit Cloud](https://streamlit.io/cloud)")
st.markdown("**ğŸ› ï¸ æŠ€è¡“æ¡†æ¶ï¼š** Streamlit + Scikit-learn + Matplotlib")
st.markdown("**âš›ï¸ æ¨¡å‹èªªæ˜ï¼š** æœ¬æ‡‰ç”¨ä½¿ç”¨å¤šå±¤æ„ŸçŸ¥å™¨ (MLP) é€²è¡Œé³¶å°¾èŠ±åˆ†é¡ï¼Œæ”¯æ´å®Œæ•´çš„è¶…åƒæ•¸èª¿æ•´èˆ‡çµæœåˆ†æ")