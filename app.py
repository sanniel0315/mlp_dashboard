import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support, roc_curve, auc
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score, learning_curve, validation_curve
from sklearn.preprocessing import StandardScaler, label_binarize
from sklearn.multiclass import OneVsRestClassifier
import os 
from matplotlib.font_manager import FontProperties, findfont, findSystemFonts, fontManager 
import joblib
from io import BytesIO
import zipfile
# --- é é¢é…ç½® ---
st.set_page_config(
    page_title="MLP æ¨¡å‹è¨“ç·´å™¨",
    page_icon="ğŸ§¬",  # DNAèºæ—‹ - è¤‡é›œå­¸ç¿’çµæ§‹çš„å®Œç¾è±¡å¾µ
    layout="wide",
    initial_sidebar_state="expanded"
)
def create_downloadable_plot(fig, filename="plot.png"):
    """å°‡ matplotlib åœ–å½¢è½‰æ›ç‚ºå¯ä¸‹è¼‰çš„æ ¼å¼"""
    buffer = BytesIO()
    fig.savefig(buffer, format='png', dpi=300, bbox_inches='tight')
    buffer.seek(0)
    return buffer
# --- æ¨¡å‹ä¿å­˜è·¯å¾‘è¨­å®š ---
MODEL_PATH = "mlp_model.pkl"
SCALER_PATH = "scaler.pkl"

# --- å­—å‹è¨­å®š ---
current_dir = os.path.dirname(__file__)
font_path = os.path.join(current_dir, "fonts", "NotoSansTC-VariableFont_wght.ttf")

if os.path.exists(font_path):
    fontManager.addfont(font_path)
    font_prop = FontProperties(fname=font_path) 
    font_family_name = font_prop.get_name() 
    
    plt.rcParams['font.family'] = font_family_name
    plt.rcParams['font.sans-serif'] = [font_family_name, 'Arial Unicode MS', 'DejaVu Sans', 'sans-serif']
    plt.rcParams['axes.unicode_minus'] = False
else:
    plt.rcParams['axes.unicode_minus'] = False 
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'sans-serif']
    
# --- æ•¸æ“šåŠ è¼‰èˆ‡é è™•ç† (ä½¿ç”¨ Streamlit ç·©å­˜) ---
@st.cache_data
def load_and_preprocess_data():
    iris = load_iris()
    X = pd.DataFrame(iris.data, columns=iris.feature_names)
    y = iris.target

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled_df = pd.DataFrame(X_scaled, columns=iris.feature_names)

    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled_df, y, test_size=0.2, random_state=42, stratify=y
    )
    return X_train, X_test, y_train, y_test, iris.target_names, X.columns.tolist(), scaler
if 'iris_original_data_loaded' not in st.session_state:
    iris_full_dataset = load_iris() # å¾ sklearn.datasets å°å…¥ load_iris
    st.session_state.original_X_df = pd.DataFrame(iris_full_dataset.data, columns=iris_full_dataset.feature_names)
    st.session_state.original_y = iris_full_dataset.target
    # target_names ä¹Ÿæ‡‰è©²åœ¨æ­¤æ™‚æˆ–é€é load_and_preprocess_data è¨­ç‚º st.session_state.target_names
    if 'target_names' not in st.session_state:
         st.session_state.target_names = iris_full_dataset.target_names.copy() # ä½¿ç”¨ .copy() é¿å…æ„å¤–ä¿®æ”¹
    st.session_state.iris_original_data_loaded = True

# --- è‡ªå®šç¾©è¨“ç·´å‡½æ•¸withçœŸå¯¦é€²åº¦ ---
def train_mlp_with_progress(mlp, X_train, y_train, progress_bar, status_text):
    """è¨“ç·´MLPä¸¦é¡¯ç¤ºé€²åº¦"""
    import time
    import warnings
    
    # çµ±ä¸€ä½¿ç”¨æ¨™æº–è¨“ç·´æ–¹æ³•ï¼Œç¢ºä¿ä¸€è‡´æ€§
    status_text.text("ğŸš€ é–‹å§‹æ¨¡å‹è¨“ç·´...")
    
    # æ¨¡æ“¬è¨“ç·´é€²åº¦
    for i in range(20, 85, 5):
        progress_bar.progress(i/100)
        time.sleep(0.1)
        status_text.text(f"ğŸƒâ€â™‚ï¸ è¨“ç·´é€²åº¦: {i}%")
    
    # å¯¦éš›è¨“ç·´ - æŠ‘åˆ¶æ”¶æ–‚è­¦å‘Š
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", message=".*Stochastic Optimizer.*")
        warnings.filterwarnings("ignore", message=".*Maximum iterations.*")
        mlp.fit(X_train, y_train)
    
    # å®Œæˆé€²åº¦
    progress_bar.progress(0.85)
    status_text.text("âœ… è¨“ç·´å®Œæˆ")
    
    return mlp

# --- è©•ä¼°å‡½æ•¸ ---
def comprehensive_evaluation(mlp, X_train, X_test, y_train, y_test, target_names):
    """ç¶œåˆè©•ä¼°æ¨¡å‹æ€§èƒ½ - ç¢ºä¿æ•¸æ“šä¸€è‡´æ€§"""
    
    # ç¢ºä¿ä½¿ç”¨æ­£ç¢ºçš„ç‰¹å¾µé€²è¡Œé æ¸¬
    y_pred_train = mlp.predict(X_train)
    y_pred_test = mlp.predict(X_test)
    y_pred_proba = mlp.predict_proba(X_test)
    
    # åŸºæœ¬æŒ‡æ¨™
    train_accuracy = accuracy_score(y_train, y_pred_train)
    test_accuracy = accuracy_score(y_test, y_pred_test)
    
    # è©³ç´°åˆ†é¡æŒ‡æ¨™ - ä½¿ç”¨æ¸¬è©¦é›†
    precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred_test, average=None)
    
    # æª¢æŸ¥æ”¶æ–‚ç‹€æ…‹
    convergence_info = {
        'converged': mlp.n_iter_ < mlp.max_iter,
        'actual_iterations': mlp.n_iter_,
        'max_iterations': mlp.max_iter
    }
    
    # äº¤å‰é©—è­‰åˆ†æ•¸ - ä½¿ç”¨èˆ‡è¨“ç·´ç›¸åŒçš„ç‰¹å¾µ
    try:
        # å‰µå»ºæ–°çš„æ¨¡å‹å¯¦ä¾‹é€²è¡Œäº¤å‰é©—è­‰ï¼Œé¿å…å½±éŸ¿å·²è¨“ç·´çš„æ¨¡å‹
        cv_model = MLPClassifier(
            hidden_layer_sizes=mlp.hidden_layer_sizes,
            activation=mlp.activation,
            solver=mlp.solver,
            alpha=mlp.alpha,
            batch_size=mlp.batch_size,
            learning_rate=mlp.learning_rate,
            learning_rate_init=mlp.learning_rate_init,
            max_iter=mlp.max_iter,
            early_stopping=mlp.early_stopping,
            validation_fraction=mlp.validation_fraction,
            n_iter_no_change=mlp.n_iter_no_change,
            tol=mlp.tol,
            random_state=42,
            verbose=False
        )
        
        # ä½¿ç”¨ç›¸åŒçš„è¨“ç·´æ•¸æ“šå’Œæ¨™ç±¤é€²è¡Œäº¤å‰é©—è­‰
        import warnings
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")  # æš«æ™‚å¿½ç•¥äº¤å‰é©—è­‰ä¸­çš„æ”¶æ–‚è­¦å‘Š
            cv_scores = cross_val_score(cv_model, X_train, y_train, cv=5, scoring='accuracy')
        
    except Exception as e:
        # å¦‚æœäº¤å‰é©—è­‰å¤±æ•—ï¼Œä½¿ç”¨æ¸¬è©¦æº–ç¢ºç‡ä½œç‚ºæ›¿ä»£
        print(f"äº¤å‰é©—è­‰å¤±æ•—: {e}")
        cv_scores = np.array([test_accuracy] * 5)  # ä½¿ç”¨æ¸¬è©¦æº–ç¢ºç‡å¡«å……
    
    return {
        'y_pred_train': y_pred_train,
        'y_pred_test': y_pred_test,
        'y_pred_proba': y_pred_proba,
        'train_accuracy': train_accuracy,
        'test_accuracy': test_accuracy,
        'cv_scores': cv_scores,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'support': support,
        'convergence_info': convergence_info
    }

# åœ¨æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚åŠ è¼‰å’Œé è™•ç†æ•¸æ“š
X_train_full, X_test_full, y_train, y_test, target_names, all_feature_names, loaded_scaler = load_and_preprocess_data()

# --- ä¸»æ¨™é¡Œ ---
st.title('ğŸ§  MLP æ¨¡å‹è¨“ç·´èˆ‡é æ¸¬ç³»çµ±')
st.markdown('### é€éèª¿æ•´åƒæ•¸è¨“ç·´ MLP æ¨¡å‹ï¼Œä¸¦å³æ™‚é€²è¡Œé æ¸¬')

# --- å´é‚Šæ¬„åƒæ•¸è¨­å®š ---
st.sidebar.header('ğŸ”§ MLP æ¨¡å‹è¶…åƒæ•¸è¨­å®š')

# ç‰¹å¾µé¸æ“‡
st.sidebar.subheader('ğŸ“Š ç‰¹å¾µé¸æ“‡')
selected_features = st.sidebar.multiselect(
    'é¸æ“‡è¦åŒ…å«çš„ç‰¹å¾µ',
    options=all_feature_names,
    default=all_feature_names
)

# åˆå§‹åŒ– session state
if 'model_trained' not in st.session_state:
    st.session_state.model_trained = False
if 'training_results' not in st.session_state:
    st.session_state.training_results = None
if 'last_selected_features' not in st.session_state:
    st.session_state.last_selected_features = None

# æª¢æŸ¥ç‰¹å¾µæ˜¯å¦æ”¹è®Šï¼ˆå¦‚æœæ”¹è®Šå‰‡æ¸…é™¤ä¹‹å‰çš„è¨“ç·´çµæœï¼‰
if (st.session_state.last_selected_features is not None and 
    st.session_state.last_selected_features != selected_features):
    st.session_state.model_trained = False
    st.session_state.training_results = None
    
st.session_state.last_selected_features = selected_features

if not selected_features:
    st.sidebar.warning("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹ç‰¹å¾µï¼")
    st.error("âš ï¸ è«‹åœ¨å·¦å´é‚Šæ¬„è‡³å°‘é¸æ“‡ä¸€å€‹ç‰¹å¾µæ‰èƒ½ç¹¼çºŒ")
    st.stop()

# æ ¹æ“šé¸æ“‡çš„ç‰¹å¾µç¯©é¸æ•¸æ“šé›†
X_train = X_train_full[selected_features]
X_test = X_test_full[selected_features]

# æ¨¡å‹è¤‡é›œåº¦
st.sidebar.subheader('ğŸ—ï¸ æ¨¡å‹è¤‡é›œåº¦')
hidden_layer_1 = st.sidebar.slider('ç¬¬ä¸€éš±è—å±¤ç¥ç¶“å…ƒæ•¸é‡', 10, 200, 100, step=10)
num_hidden_layers = st.sidebar.radio('éš±è—å±¤æ•¸é‡', [1, 2], index=0)

hidden_layer_sizes = (hidden_layer_1,)
if num_hidden_layers == 2:
    hidden_layer_2 = st.sidebar.slider('ç¬¬äºŒéš±è—å±¤ç¥ç¶“å…ƒæ•¸é‡', 10, 100, 50, step=10)
    hidden_layer_sizes = (hidden_layer_1, hidden_layer_2)

st.sidebar.write(f'ğŸ”¹ éš±è—å±¤çµæ§‹: {hidden_layer_sizes}')

# æ´»åŒ–å‡½æ•¸
activation_function = st.sidebar.selectbox(
    'âš¡ æ´»åŒ–å‡½æ•¸',
    ['relu', 'logistic', 'tanh', 'identity'],
    index=0
)

# å„ªåŒ–å™¨
solver = st.sidebar.selectbox(
    'ğŸš€ å„ªåŒ–å™¨',
    ['adam', 'sgd', 'lbfgs'],
    index=0
)

# å­¸ç¿’ç‡è¨­å®š
learning_rate_init = 0.001
if solver in ['adam', 'sgd']:
    st.sidebar.subheader('ğŸ“ˆ å­¸ç¿’ç‡è¨­å®š')
    learning_rate = st.sidebar.selectbox(
        'å­¸ç¿’ç‡ç­–ç•¥',
        ['constant', 'invscaling', 'adaptive'],
        index=0
    )
    learning_rate_init = st.sidebar.number_input('åˆå§‹å­¸ç¿’ç‡', min_value=0.0001, max_value=0.1, value=0.001, step=0.0001, format="%.4f")
else:
    learning_rate = 'constant'

# æ‰¹æ¬¡å¤§å°
batch_size = 'auto'
if solver in ['adam', 'sgd']:
    st.sidebar.subheader('ğŸ“¦ æ‰¹æ¬¡å¤§å°')
    batch_size_option = st.sidebar.radio('æ‰¹æ¬¡å¤§å°è¨­å®š', ['auto', 'æ‰‹å‹•è¼¸å…¥'], index=0)
    if batch_size_option == 'æ‰‹å‹•è¼¸å…¥':
        batch_size = st.sidebar.number_input('Batch Size', min_value=1, max_value=len(X_train), value=32, step=1)

# è¨“ç·´åƒæ•¸
st.sidebar.subheader('ğŸ¯ è¨“ç·´åƒæ•¸')
max_iter = st.sidebar.slider('æœ€å¤§è¿­ä»£æ¬¡æ•¸', 50, 1000, 200, step=50)

early_stopping = st.sidebar.checkbox('å•Ÿç”¨ Early Stopping', value=False)
validation_fraction = 0.1
n_iter_no_change = 10
tol = 1e-4

if early_stopping:
    validation_fraction = st.sidebar.slider('é©—è­‰é›†æ¯”ä¾‹', 0.05, 0.5, 0.1, step=0.05)
    n_iter_no_change = st.sidebar.slider('ç„¡æ”¹å–„å®¹å¿è¿­ä»£æ¬¡æ•¸', 10, 100, 10, step=5)
    tol = st.sidebar.number_input('å®¹å¿åº¦', min_value=1e-5, max_value=1e-2, value=1e-4, format="%.5f")

# æ­£è¦åŒ–åƒæ•¸
alpha = st.sidebar.number_input('ğŸ›¡ï¸ L2 æ­£å‰‡åŒ–å¼·åº¦', min_value=0.0001, max_value=1.0, value=0.0001, step=0.0001, format="%.4f")

# æª¢æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„æ¨¡å‹
model_exists = os.path.exists(MODEL_PATH) and os.path.exists(SCALER_PATH)
if model_exists:
    st.sidebar.success("âœ… ç™¼ç¾å·²ä¿å­˜çš„æ¨¡å‹")
else:
    st.sidebar.info("â„¹ï¸ å°šæœªè¨“ç·´æ¨¡å‹")

# åƒæ•¸å»ºè­°
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ’¡ åƒæ•¸èª¿å„ªå»ºè­°")

if len(selected_features) < 4:
    st.sidebar.warning("ç‰¹å¾µæ•¸é‡è¼ƒå°‘ï¼Œå»ºè­°ä½¿ç”¨è¼ƒå°çš„éš±è—å±¤")
    
if solver == 'lbfgs' and max_iter > 500:
    st.sidebar.info("L-BFGS é€šå¸¸æ”¶æ–‚è¼ƒå¿«ï¼Œå¯å˜—è©¦è¼ƒå°‘è¿­ä»£æ¬¡æ•¸")
elif solver in ['adam', 'sgd'] and max_iter < 500:
    st.sidebar.warning("Adam/SGD å¯èƒ½éœ€è¦æ›´å¤šè¿­ä»£æ¬¡æ•¸é¿å…æ”¶æ–‚è­¦å‘Š")
    
if solver in ['adam', 'sgd'] and learning_rate_init > 0.01:
    st.sidebar.warning("å­¸ç¿’ç‡è¼ƒé«˜ï¼Œå¯èƒ½å°è‡´è¨“ç·´ä¸ç©©å®šæˆ–é›£ä»¥æ”¶æ–‚")

if hidden_layer_1 > 150 and len(selected_features) <= 4:
    st.sidebar.warning("éš±è—å±¤ç¥ç¶“å…ƒæ•¸å¯èƒ½éå¤šï¼Œæ˜“éæ“¬åˆ")

# æ”¶æ–‚å„ªåŒ–å»ºè­°
st.sidebar.markdown("**ğŸ”„ æ”¶æ–‚å„ªåŒ–ï¼š**")
st.sidebar.write("â€¢ é‡åˆ°æ”¶æ–‚è­¦å‘Šæ™‚å¢åŠ è¿­ä»£æ¬¡æ•¸")
st.sidebar.write("â€¢ å•Ÿç”¨ Early Stopping é˜²æ­¢éåº¦è¨“ç·´")
st.sidebar.write("â€¢ èª¿ä½å­¸ç¿’ç‡æé«˜ç©©å®šæ€§")

# ç•¶å‰ç‹€æ…‹é¡¯ç¤º
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ“‹ ç•¶å‰ç‹€æ…‹")

if st.session_state.model_trained and st.session_state.training_results:
    results = st.session_state.training_results
    st.sidebar.success("âœ… æ¨¡å‹å·²è¨“ç·´")
    st.sidebar.metric("æ¸¬è©¦æº–ç¢ºç‡", f"{results['test_accuracy']:.3f}")
    st.sidebar.metric("F1-Score", f"{results['f1'].mean():.3f}")
else:
    st.sidebar.info("â³ ç­‰å¾…è¨“ç·´")

# æ•¸æ“šé›†ä¿¡æ¯
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ“Š æ•¸æ“šé›†è³‡è¨Š")
st.sidebar.write(f"â€¢ ç¸½æ¨£æœ¬æ•¸: {len(X_train_full) + len(X_test_full)}")
st.sidebar.write(f"â€¢ è¨“ç·´é›†: {len(X_train_full)} æ¨£æœ¬")
st.sidebar.write(f"â€¢ æ¸¬è©¦é›†: {len(X_test_full)} æ¨£æœ¬")
st.sidebar.write(f"â€¢ ç‰¹å¾µç¸½æ•¸: {len(all_feature_names)}")
st.sidebar.write(f"â€¢ é¸æ“‡ç‰¹å¾µ: {len(selected_features)}")
st.sidebar.write(f"â€¢ é¡åˆ¥æ•¸: {len(target_names)}")

# --- ä¸»è¦å…§å®¹å€åŸŸä½¿ç”¨ Tabs ---
tab1, tab2, tab3 = st.tabs(["ğŸ¯ æ¨¡å‹è¨“ç·´", "ğŸ“Š è¨“ç·´çµæœ", "ğŸ”® å³æ™‚é æ¸¬"])

# --- Tab 1: æ¨¡å‹è¨“ç·´ ---
with tab1:
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ”§ ç•¶å‰æ¨¡å‹è¨­å®š")
        
        # åƒæ•¸å±•ç¤º
        param_col1, param_col2 = st.columns(2)
        with param_col1:
            st.write("**ğŸ—ï¸ æ¨¡å‹çµæ§‹**")
            st.write(f"â€¢ ç‰¹å¾µæ•¸é‡: {len(selected_features)}")
            st.write(f"â€¢ éš±è—å±¤: {hidden_layer_sizes}")
            st.write(f"â€¢ æ´»åŒ–å‡½æ•¸: {activation_function}")
            st.write(f"â€¢ å„ªåŒ–å™¨: {solver}")
            
        with param_col2:
            st.write("**âš™ï¸ è¨“ç·´åƒæ•¸**")
            st.write(f"â€¢ æœ€å¤§è¿­ä»£æ¬¡æ•¸: {max_iter}")
            st.write(f"â€¢ Early Stopping: {'å•Ÿç”¨' if early_stopping else 'åœç”¨'}")
            st.write(f"â€¢ L2 æ­£å‰‡åŒ–: {alpha}")
            if solver in ['adam', 'sgd']:
                st.write(f"â€¢ å­¸ç¿’ç‡: {learning_rate_init}")
        
        st.write("**ğŸ“‹ é¸æ“‡çš„ç‰¹å¾µ:**")
        st.write(", ".join(selected_features))
    
    with col2:
        st.subheader("ğŸš€ æ¨¡å‹æ§åˆ¶")
        
        train_col, reset_col = st.columns(2)
        
        with train_col:
            if st.button('ğŸ¯ è¨“ç·´æ¨¡å‹', type="primary", use_container_width=True):
                if not selected_features:
                    st.error("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹ç‰¹å¾µæ‰èƒ½è¨“ç·´æ¨¡å‹ï¼")
                else:
                    try:
                        # å»ºç«‹é€²åº¦è¿½è¹¤å®¹å™¨
                        progress_container = st.container()
                        with progress_container:
                            progress_bar = st.progress(0)
                            status_text = st.empty()
                        
                        status_text.text("ğŸ”§ å»ºç«‹æ¨¡å‹æ¶æ§‹...")
                        progress_bar.progress(10)
                        
                        # æ•¸æ“šé©—è­‰
                        st.write(f"ğŸ“Š **è¨“ç·´æ•¸æ“šè³‡è¨Š:**")
                        st.write(f"â€¢ é¸æ“‡ç‰¹å¾µ: {len(selected_features)} å€‹")
                        st.write(f"â€¢ è¨“ç·´æ¨£æœ¬: {X_train.shape[0]} å€‹")
                        st.write(f"â€¢ æ¸¬è©¦æ¨£æœ¬: {X_test.shape[0]} å€‹")
                        st.write(f"â€¢ é¡åˆ¥åˆ†å¸ƒ: {dict(zip(target_names, np.bincount(y_train)))}")
                        
                        # å»ºç«‹æ¨¡å‹
                        mlp = MLPClassifier(
                            hidden_layer_sizes=hidden_layer_sizes,
                            activation=activation_function,
                            solver=solver,
                            alpha=alpha,
                            batch_size=batch_size,
                            learning_rate=learning_rate,
                            learning_rate_init=learning_rate_init,
                            max_iter=max_iter,
                            early_stopping=early_stopping,
                            validation_fraction=validation_fraction,
                            n_iter_no_change=n_iter_no_change,
                            tol=tol,
                            random_state=42,
                            verbose=False  # é—œé–‰è©³ç´°è¼¸å‡ºï¼Œä½¿ç”¨è‡ªå®šç¾©é€²åº¦
                        )
                        
                        progress_bar.progress(20)
                        
                        # ä½¿ç”¨è‡ªå®šç¾©è¨“ç·´å‡½æ•¸
                        mlp = train_mlp_with_progress(mlp, X_train, y_train, progress_bar, status_text)
                        
                        status_text.text("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
                        progress_bar.progress(90)
                        
                        # ä¿å­˜æ¨¡å‹
                        joblib.dump(mlp, MODEL_PATH)
                        joblib.dump(loaded_scaler, SCALER_PATH)
                        
                        status_text.text("ğŸ“Š ç¶œåˆè©•ä¼°æ¨¡å‹...")
                        progress_bar.progress(95)
                        
                        # ç¶œåˆè©•ä¼°
                        evaluation_results = comprehensive_evaluation(mlp, X_train, X_test, y_train, y_test, target_names)
                        
                        # ä¿å­˜çµæœåˆ° session state
                        st.session_state.training_results = {
                            'mlp': mlp,
                            'selected_features': selected_features,
                            **evaluation_results
                        }
                        st.session_state.model_trained = True
                        
                        progress_bar.progress(100)
                        status_text.text("âœ… è¨“ç·´å®Œæˆï¼")
                        
                        # é¡¯ç¤ºå¿«é€Ÿæ‘˜è¦
                        st.success("ğŸ‰ æ¨¡å‹è¨“ç·´æˆåŠŸï¼")
                        st.info("ğŸ’¡ åˆ‡æ›åˆ°ã€ŒğŸ“Š è¨“ç·´çµæœã€æŸ¥çœ‹è©³ç´°åˆ†æï¼Œæˆ–åˆ°ã€ŒğŸ”® å³æ™‚é æ¸¬ã€é€²è¡Œé æ¸¬ã€‚")
                        
                    except Exception as e:
                        st.error(f"âŒ è¨“ç·´éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                        import traceback
                        st.text("è©³ç´°éŒ¯èª¤ä¿¡æ¯ï¼š")
                        st.code(traceback.format_exc())
        
        with reset_col:
            if st.button('ğŸ”„ é‡ç½®æ¨¡å‹', use_container_width=True):
                # æ¸…é™¤ session state
                if 'model_trained' in st.session_state:
                    del st.session_state.model_trained
                if 'training_results' in st.session_state:
                    del st.session_state.training_results
                
                # åˆªé™¤ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶
                try:
                    if os.path.exists(MODEL_PATH):
                        os.remove(MODEL_PATH)
                    if os.path.exists(SCALER_PATH):
                        os.remove(SCALER_PATH)
                    st.success("âœ… æ¨¡å‹å·²é‡ç½®ï¼")
                except:
                    st.warning("âš ï¸ æ¨¡å‹æ–‡ä»¶åˆªé™¤å¤±æ•—ï¼Œä½†è¨˜æ†¶å·²æ¸…é™¤")
                
                st.rerun()  # é‡æ–°é‹è¡Œæ‡‰ç”¨ç¨‹å¼
    
    # åœ¨æ‰€æœ‰ columns å¤–é¢é¡¯ç¤ºå¿«é€Ÿçµæœé è¦½
    if st.session_state.model_trained and st.session_state.training_results:
        st.subheader("ğŸ“Š å¿«é€Ÿçµæœé è¦½")
        preview_col1, preview_col2, preview_col3 = st.columns(3)
        
        evaluation_results = st.session_state.training_results
        
        with preview_col1:
            st.metric("ğŸ¯ æ¸¬è©¦æº–ç¢ºç‡", f"{evaluation_results['test_accuracy']:.3f}")
        with preview_col2:
            st.metric("ğŸ“Š äº¤å‰é©—è­‰", f"{evaluation_results['cv_scores'].mean():.3f}")
        with preview_col3:
            overfitting = evaluation_results['train_accuracy'] - evaluation_results['test_accuracy']
            st.metric("ğŸ” éæ“¬åˆç¨‹åº¦", f"{overfitting:.3f}")
        
        # çµæœåˆç†æ€§æª¢æŸ¥
        if evaluation_results['test_accuracy'] < 0.6:
            st.warning("âš ï¸ æ¸¬è©¦æº–ç¢ºç‡è¼ƒä½ï¼Œå»ºè­°èª¿æ•´è¶…åƒæ•¸æˆ–æª¢æŸ¥ç‰¹å¾µé¸æ“‡")
        
        if abs(evaluation_results['cv_scores'].mean() - evaluation_results['test_accuracy']) > 0.2:
            st.warning("âš ï¸ äº¤å‰é©—è­‰èˆ‡æ¸¬è©¦çµæœå·®ç•°è¼ƒå¤§ï¼Œå¯èƒ½å­˜åœ¨æ•¸æ“šæ´©éœ²æˆ–éæ“¬åˆ")

# --- Tab 2: è¨“ç·´çµæœ ---
with tab2:
    if st.session_state.model_trained and st.session_state.training_results:
        results = st.session_state.training_results
        
        # === æ¨¡å‹æ€§èƒ½ç¸½è¦½ ===
        st.subheader("ğŸ¯ æ¨¡å‹æ€§èƒ½ç¸½è¦½")
        
        # ä¸»è¦æŒ‡æ¨™ - æ”¹ç‚ºå‚ç›´æ’åˆ—é¿å…éå¤šåˆ—
        col1, col2 = st.columns(2)
        
        with col1:
            st.metric(
                "ğŸ¯ æ¸¬è©¦æº–ç¢ºç‡", 
                f"{results['test_accuracy']:.3f}",
                delta=f"{(results['test_accuracy'] - 0.33):.3f}"
            )
            cv_mean = results['cv_scores'].mean()
            cv_std = results['cv_scores'].std()
            st.metric(
                "ğŸ“Š äº¤å‰é©—è­‰", 
                f"{cv_mean:.3f}",
                delta=f"Â±{cv_std:.3f}"
            )
            
        with col2:
            overfitting = results['train_accuracy'] - results['test_accuracy']
            overfitting_status = "ğŸŸ¢ æ­£å¸¸" if abs(overfitting) < 0.05 else ("ğŸŸ¡ è¼•å¾®" if overfitting < 0.15 else "ğŸ”´ åš´é‡")
            st.metric(
                "ğŸ” éæ“¬åˆæª¢æ¸¬", 
                f"{overfitting:.3f}",
                delta=overfitting_status
            )
            # F1 score macro average
            f1_macro = results['f1'].mean()
            f1_status = "ğŸŸ¢ å„ªç§€" if f1_macro > 0.9 else ("ğŸŸ¡ è‰¯å¥½" if f1_macro > 0.7 else "ğŸ”´ éœ€æ”¹é€²")
            st.metric(
                "âš¡ F1-Score", 
                f"{f1_macro:.3f}",
                delta=f1_status
            )
        
        # === æ€§èƒ½è¨ºæ–· ===
        st.subheader("ğŸ”§ æ€§èƒ½è¨ºæ–·")
        
        diagnosis_col1, diagnosis_col2 = st.columns(2)
        
        with diagnosis_col1:
            st.write("**ğŸ©º æ¨¡å‹å¥åº·æª¢æŸ¥**")
            
            # æ”¶æ–‚ç‹€æ…‹æª¢æŸ¥
            convergence = results['convergence_info']
            if convergence['converged']:
                st.success(f"âœ… æ¨¡å‹å·²æ”¶æ–‚ ({convergence['actual_iterations']}/{convergence['max_iterations']} è¿­ä»£)")
            else:
                st.error(f"âŒ æ¨¡å‹æœªæ”¶æ–‚ ({convergence['actual_iterations']}/{convergence['max_iterations']} è¿­ä»£)")
            
            # æº–ç¢ºç‡æª¢æŸ¥
            if results['test_accuracy'] > 0.9:
                st.success("âœ… æº–ç¢ºç‡å„ªç§€")
            elif results['test_accuracy'] > 0.7:
                st.info("â„¹ï¸ æº–ç¢ºç‡è‰¯å¥½")
            else:
                st.error("âŒ æº–ç¢ºç‡åä½ï¼Œéœ€è¦èª¿å„ª")
            
            # éæ“¬åˆæª¢æŸ¥
            overfitting = results['train_accuracy'] - results['test_accuracy']
            if abs(overfitting) < 0.05:
                st.success("âœ… ç„¡æ˜é¡¯éæ“¬åˆ")
            elif overfitting > 0.15:
                st.warning("âš ï¸ å¯èƒ½å­˜åœ¨éæ“¬åˆ")
            elif overfitting < -0.05:
                st.warning("âš ï¸ ç•°å¸¸ï¼šæ¸¬è©¦é›†è¡¨ç¾å„ªæ–¼è¨“ç·´é›†")
            
            # äº¤å‰é©—è­‰ä¸€è‡´æ€§æª¢æŸ¥
            cv_test_diff = abs(results['cv_scores'].mean() - results['test_accuracy'])
            if cv_test_diff < 0.1:
                st.success("âœ… äº¤å‰é©—è­‰çµæœä¸€è‡´")
            else:
                st.warning(f"âš ï¸ äº¤å‰é©—è­‰èˆ‡æ¸¬è©¦çµæœå·®ç•°è¼ƒå¤§ ({cv_test_diff:.3f})")
        
        with diagnosis_col2:
            st.write("**ğŸ’Š æ”¹é€²å»ºè­°**")
            
            suggestions = []
            
            # æ”¶æ–‚å•é¡Œå»ºè­°
            convergence = results['convergence_info']
            if not convergence['converged']:
                suggestions.append("ğŸ”„ **æ”¶æ–‚å•é¡Œï¼š**")
                suggestions.append("â€¢ å¢åŠ æœ€å¤§è¿­ä»£æ¬¡æ•¸ (1000-2000)")
                suggestions.append("â€¢ é™ä½å­¸ç¿’ç‡ (0.001 â†’ 0.0001)")
                suggestions.append("â€¢ å•Ÿç”¨ Early Stopping")
                suggestions.append("â€¢ èª¿æ•´å®¹å¿åº¦ (1e-4 â†’ 1e-6)")
                suggestions.append("")
            
            if results['test_accuracy'] < 0.7:
                suggestions.append("ğŸ¯ **æº–ç¢ºç‡æå‡ï¼š**")
                suggestions.append("â€¢ å˜—è©¦å¢åŠ éš±è—å±¤ç¥ç¶“å…ƒæ•¸é‡")
                suggestions.append("â€¢ èª¿æ•´å­¸ç¿’ç‡æˆ–å„ªåŒ–å™¨")
                suggestions.append("â€¢ å¢åŠ ç‰¹å¾µæˆ–æª¢æŸ¥ç‰¹å¾µå“è³ª")
                suggestions.append("")
            
            overfitting = results['train_accuracy'] - results['test_accuracy']
            if overfitting > 0.15:
                suggestions.append("ğŸ›¡ï¸ **éæ“¬åˆè§£æ±ºï¼š**")
                suggestions.append("â€¢ å¢åŠ  L2 æ­£å‰‡åŒ–å¼·åº¦ (alpha)")
                suggestions.append("â€¢ å•Ÿç”¨ Early Stopping")
                suggestions.append("â€¢ æ¸›å°‘éš±è—å±¤å¤§å°")
                suggestions.append("")
            
            if results['cv_scores'].std() > 0.1:
                suggestions.append("ğŸ“Š **ç©©å®šæ€§æ”¹å–„ï¼š**")
                suggestions.append("â€¢ æ¨¡å‹ä¸å¤ ç©©å®šï¼Œå˜—è©¦ä¸åŒçš„éš¨æ©Ÿç¨®å­")
                suggestions.append("â€¢ è€ƒæ…®ä½¿ç”¨æ›´ä¿å®ˆçš„åƒæ•¸")
                suggestions.append("")
            
            if results['f1'].min() < 0.5:
                suggestions.append("âš–ï¸ **é¡åˆ¥å¹³è¡¡ï¼š**")
                suggestions.append("â€¢ æŸäº›é¡åˆ¥è­˜åˆ¥æ•ˆæœå·®")
                suggestions.append("â€¢ æª¢æŸ¥æ•¸æ“šæ˜¯å¦ä¸å¹³è¡¡")
            
            if not suggestions:
                st.success("ğŸ‰ æ¨¡å‹è¡¨ç¾è‰¯å¥½ï¼Œç„¡éœ€ç‰¹åˆ¥èª¿æ•´ï¼")
            else:
                for suggestion in suggestions:
                    if suggestion.startswith("ğŸ”„") or suggestion.startswith("ğŸ¯") or suggestion.startswith("ğŸ›¡ï¸") or suggestion.startswith("ğŸ“Š") or suggestion.startswith("âš–ï¸"):
                        st.write(f"**{suggestion}**")
                    elif suggestion == "":
                        continue
                    else:
                        st.write(suggestion)
        
        # === è©³ç´°è©•ä¼°æŒ‡æ¨™ ===
        st.subheader("ğŸ“ˆ è©³ç´°è©•ä¼°æŒ‡æ¨™")
        
        eval_col1, eval_col2 = st.columns(2)
        
        with eval_col1:
            st.write("**ğŸ² äº¤å‰é©—è­‰åˆ†æ•¸åˆ†æ**")
            cv_df = pd.DataFrame({
                'Fold': [f'Fold {i+1}' for i in range(len(results['cv_scores']))],
                'æº–ç¢ºç‡': results['cv_scores']
            })
            
            # äº¤å‰é©—è­‰çµæœåœ–è¡¨
            fig_cv, ax_cv = plt.subplots(figsize=(8, 4))
            bars = ax_cv.bar(cv_df['Fold'], cv_df['æº–ç¢ºç‡'], color='skyblue', alpha=0.7)
            ax_cv.axhline(y=cv_df['æº–ç¢ºç‡'].mean(), color='red', linestyle='--', 
                         label=f'å¹³å‡å€¼: {cv_df["æº–ç¢ºç‡"].mean():.3f}')
            ax_cv.set_ylabel('æº–ç¢ºç‡')
            ax_cv.set_title('5-Fold äº¤å‰é©—è­‰çµæœ')
            ax_cv.legend()
            ax_cv.grid(True, alpha=0.3)
            
            # åœ¨æŸ±ç‹€åœ–ä¸Šé¡¯ç¤ºæ•¸å€¼
            for bar, score in zip(bars, results['cv_scores']):
                height = bar.get_height()
                ax_cv.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                          f'{score:.3f}', ha='center', va='bottom')
            
            st.pyplot(fig_cv)
            # æ·»åŠ ä¸‹è¼‰æŒ‰éˆ•
            buffer = create_downloadable_plot(fig_cv, "cross_validation_results.png")
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰äº¤å‰é©—è­‰åœ–è¡¨",
                data=buffer,
                file_name="cross_validation_results.png",
                mime="image/png"
            )
            
            plt.close(fig_cv)
        
        with eval_col2:
            st.write("**ğŸ“Š å„é¡åˆ¥æ€§èƒ½æŒ‡æ¨™**")
            # å‰µå»ºè©³ç´°çš„æ€§èƒ½æŒ‡æ¨™è¡¨
            performance_df = pd.DataFrame({
                'é¡åˆ¥': target_names,
                'ç²¾ç¢ºç‡': [f"{p:.3f}" for p in results['precision']],
                'å¬å›ç‡': [f"{r:.3f}" for r in results['recall']],
                'F1-Score': [f"{f:.3f}" for f in results['f1']],
                'æ¨£æœ¬æ•¸': results['support'].astype(int)
            })
            st.dataframe(performance_df, use_container_width=True)
        
        # æ€§èƒ½æŒ‡æ¨™é›·é”åœ– - ç§»åˆ°columnså¤–é¢ä»¥é¿å…åµŒå¥—
        st.write("**ğŸ“Š å„é¡åˆ¥æ€§èƒ½æŒ‡æ¨™é›·é”åœ–**")
        categories = target_names
        fig_radar, ax_radar = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))
        
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False)
        angles = np.concatenate((angles, [angles[0]]))
        
        # ç¹ªè£½ç²¾ç¢ºç‡ã€å¬å›ç‡ã€F1-Score
        for metric_name, metric_values, color in [
            ('ç²¾ç¢ºç‡', results['precision'], 'blue'),
            ('å¬å›ç‡', results['recall'], 'green'),
            ('F1-Score', results['f1'], 'red')
        ]:
            values = np.concatenate((metric_values, [metric_values[0]]))
            ax_radar.plot(angles, values, 'o-', linewidth=2, label=metric_name, color=color)
            ax_radar.fill(angles, values, alpha=0.25, color=color)
        
        ax_radar.set_xticks(angles[:-1])
        ax_radar.set_xticklabels(categories)
        ax_radar.set_ylim(0, 1)
        ax_radar.set_title('å„é¡åˆ¥æ€§èƒ½æŒ‡æ¨™é›·é”åœ–', pad=20)
        ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax_radar.grid(True)
        
        st.pyplot(fig_radar)
        buffer = create_downloadable_plot(fig_radar, "performance_radar_chart.png")
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰é›·é”åœ–",
            data=buffer,
            file_name="performance_radar_chart.png",
            mime="image/png"
        )

        plt.close(fig_radar)
       
        # === è¦–è¦ºåŒ–åˆ†æ ===
        st.subheader("ğŸ“Š è¦–è¦ºåŒ–åˆ†æ")
        
        visual_col1, visual_col2 = st.columns(2)
        
        with visual_col1:
            st.write("**ğŸ”¥ æ··æ·†çŸ©é™£**")
            cm = confusion_matrix(y_test, results['y_pred_test'])
            fig_cm, ax_cm = plt.subplots(figsize=(6, 5))
            
            # è¨ˆç®—ç™¾åˆ†æ¯”
            cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            
            # å‰µå»ºæ¨™è¨»
            annot = []
            for i in range(cm.shape[0]):
                row = []
                for j in range(cm.shape[1]):
                    row.append(f'{cm[i,j]}\n({cm_percent[i,j]:.1%})')
                annot.append(row)
            
            sns.heatmap(cm, annot=annot, fmt='', cmap='Blues',
                        xticklabels=target_names, yticklabels=target_names, ax=ax_cm)
            ax_cm.set_xlabel('é æ¸¬æ¨™ç±¤')
            ax_cm.set_ylabel('çœŸå¯¦æ¨™ç±¤')
            ax_cm.set_title('æ··æ·†çŸ©é™£ (æ•¸é‡ & ç™¾åˆ†æ¯”)')
            st.pyplot(fig_cm)
            plt.close(fig_cm)
            # æ·»åŠ ä¸‹è¼‰æŒ‰éˆ•
            buffer = create_downloadable_plot(fig_cm, "confusion_matrix.png")
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰æ··æ·†çŸ©é™£",
                data=buffer,
                file_name="confusion_matrix.png",
                mime="image/png"
            )
            
            plt.close(fig_cm)
        with visual_col2:
            st.write("**ğŸ¯ é æ¸¬ä¿¡å¿ƒåº¦åˆ†æ**")
            # åˆ†æé æ¸¬ä¿¡å¿ƒåº¦åˆ†å¸ƒ
            max_probas = np.max(results['y_pred_proba'], axis=1)
            
            fig_conf, ax_conf = plt.subplots(figsize=(6, 5))
            n, bins, patches = ax_conf.hist(max_probas, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
            ax_conf.axvline(x=max_probas.mean(), color='red', linestyle='--', 
                           label=f'å¹³å‡ä¿¡å¿ƒåº¦: {max_probas.mean():.3f}')
            ax_conf.set_xlabel('æœ€å¤§é æ¸¬æ©Ÿç‡')
            ax_conf.set_ylabel('æ¨£æœ¬æ•¸')
            ax_conf.set_title('æ¨¡å‹é æ¸¬ä¿¡å¿ƒåº¦åˆ†å¸ƒ')
            ax_conf.legend()
            ax_conf.grid(True, alpha=0.3)
            
            st.pyplot(fig_conf)
            buffer = create_downloadable_plot(fig_conf, "confidence_distribution.png")
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰ä¿¡å¿ƒåº¦åˆ†å¸ƒåœ–",
                data=buffer,
                file_name="confidence_distribution.png",
                mime="image/png"
            )
            
            plt.close(fig_conf)

        
        st.write("**ä¿¡å¿ƒåº¦çµ±è¨ˆï¼š**")
        max_probas = np.max(results['y_pred_proba'], axis=1)
        confidence_stats = pd.DataFrame({
            'æŒ‡æ¨™': ['å¹³å‡ä¿¡å¿ƒåº¦', 'æœ€ä½ä¿¡å¿ƒåº¦', 'æœ€é«˜ä¿¡å¿ƒåº¦', 'æ¨™æº–å·®'],
            'æ•¸å€¼': [f"{max_probas.mean():.3f}", f"{max_probas.min():.3f}", 
                    f"{max_probas.max():.3f}", f"{max_probas.std():.3f}"]
        })
        st.dataframe(confidence_stats, use_container_width=True)
        
        # === å­¸ç¿’æ›²ç·šåˆ†æ ===
        if hasattr(results['mlp'], 'loss_curve_') and results['mlp'].loss_curve_:
            st.subheader("ğŸ“‰ å­¸ç¿’æ›²ç·šåˆ†æ")
            
            st.write("**ğŸ”» è¨“ç·´æå¤±æ›²ç·š**")
            fig_loss, ax_loss = plt.subplots(figsize=(10, 5))
            ax_loss.plot(results['mlp'].loss_curve_, 'b-', linewidth=2, label='è¨“ç·´æå¤±')
            ax_loss.set_xlabel('è¿­ä»£æ¬¡æ•¸')
            ax_loss.set_ylabel('æå¤±å€¼')
            ax_loss.set_title('è¨“ç·´éç¨‹æå¤±è®ŠåŒ–')
            ax_loss.grid(True, alpha=0.3)
            ax_loss.legend()
            
            # æ¨™è¨»é‡è¦é»
            min_loss_idx = np.argmin(results['mlp'].loss_curve_)
            min_loss_val = results['mlp'].loss_curve_[min_loss_idx]
            ax_loss.plot(min_loss_idx, min_loss_val, 'ro', markersize=8, 
                       label=f'æœ€ä½æå¤±: {min_loss_val:.4f}')
            ax_loss.legend()
            
            st.pyplot(fig_loss)
            buffer = create_downloadable_plot(fig_loss, "learning_curve.png")
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰å­¸ç¿’æ›²ç·š",
                data=buffer,
                file_name="learning_curve.png",
                mime="image/png"
            )
            
            plt.close(fig_loss)
                        
            # å­¸ç¿’çµ±è¨ˆ
            st.write("**ğŸ“Š å­¸ç¿’çµ±è¨ˆ**")
            loss_curve = results['mlp'].loss_curve_
            learning_stats = pd.DataFrame({
                'æŒ‡æ¨™': ['æœ€çµ‚æå¤±', 'æœ€ä½æå¤±', 'åˆå§‹æå¤±', 'æå¤±ä¸‹é™ç‡'],
                'æ•¸å€¼': [
                    f"{loss_curve[-1]:.4f}",
                    f"{np.min(loss_curve):.4f}",
                    f"{loss_curve[0]:.4f}",
                    f"{((loss_curve[0] - loss_curve[-1]) / loss_curve[0] * 100):.1f}%"
                ]
            })
            st.dataframe(learning_stats, use_container_width=True)
            
            # æ”¶æ–‚æ€§åˆ†æ
            st.write("**ğŸ“ˆ æ”¶æ–‚æ€§åˆ†æï¼š**")
            recent_losses = loss_curve[-10:] if len(loss_curve) >= 10 else loss_curve
            loss_variance = np.var(recent_losses)
            
            if loss_variance < 1e-6:
                st.success("âœ… æ¨¡å‹å·²è‰¯å¥½æ”¶æ–‚")
            elif loss_variance < 1e-4:
                st.info("â„¹ï¸ æ¨¡å‹åŸºæœ¬æ”¶æ–‚")
            else:
                st.warning("âš ï¸ æ¨¡å‹å¯èƒ½éœ€è¦æ›´å¤šè¿­ä»£")
            
            st.write(f"æœ€å¾Œ10æ¬¡è¿­ä»£çš„æå¤±æ–¹å·®: {loss_variance:.2e}")
        
        # === å¿«é€Ÿæ“ä½œ ===
        st.subheader("âš¡ å¿«é€Ÿæ“ä½œ")
        
        if st.button("ğŸ”„ é‡æ–°è¨“ç·´", type="secondary", use_container_width=False):
            st.info("ğŸ’¡ è«‹åˆ‡æ›åˆ°ã€ŒğŸ¯ æ¨¡å‹è¨“ç·´ã€æ¨™ç±¤é èª¿æ•´åƒæ•¸ä¸¦é‡æ–°è¨“ç·´")
        
        if st.button("ğŸ”® å‰å¾€é æ¸¬", type="secondary", use_container_width=False):
            st.info("ğŸ’¡ è«‹åˆ‡æ›åˆ°ã€ŒğŸ”® å³æ™‚é æ¸¬ã€æ¨™ç±¤é é€²è¡Œé æ¸¬")
        
        if st.button("ğŸ“¥ ä¸‹è¼‰å ±å‘Š", type="secondary", use_container_width=False):
            # å‰µå»ºç°¡å–®çš„å ±å‘Šæ‘˜è¦
            report_data = {
                "æ¸¬è©¦æº–ç¢ºç‡": results['test_accuracy'],
                "äº¤å‰é©—è­‰å‡å€¼": results['cv_scores'].mean(),
                "äº¤å‰é©—è­‰æ¨™æº–å·®": results['cv_scores'].std(),
                "F1-Score": results['f1'].mean(),
                "éæ“¬åˆç¨‹åº¦": results['train_accuracy'] - results['test_accuracy']
            }
            
            report_df = pd.DataFrame(list(report_data.items()), columns=['æŒ‡æ¨™', 'æ•¸å€¼'])
            csv = report_df.to_csv(index=False)
            st.download_button(
                label="ä¸‹è¼‰ CSV å ±å‘Š",
                data=csv,
                file_name="mlp_training_report.csv",
                mime="text/csv"
            )
        
        # === æ¨¡å‹è¤‡é›œåº¦åˆ†æ ===
        st.subheader("ğŸ” æ¨¡å‹è©³ç´°è³‡è¨Š")
        
        model_info_col1, model_info_col2 = st.columns(2)
        
        with model_info_col1:
            st.write("**ğŸ—ï¸ æ¨¡å‹æ¶æ§‹**")
            architecture_info = pd.DataFrame({
                'å±¤ç´š': ['è¼¸å…¥å±¤'] + [f'éš±è—å±¤{i+1}' for i in range(len(results['mlp'].coefs_)-1)] + ['è¼¸å‡ºå±¤'],
                'ç¥ç¶“å…ƒæ•¸': [results['mlp'].coefs_[0].shape[0]] + 
                          [coef.shape[1] for coef in results['mlp'].coefs_[:-1]] + 
                          [results['mlp'].coefs_[-1].shape[1]]
            })
            st.dataframe(architecture_info, use_container_width=True)
        
        with model_info_col2:
            st.write("**ğŸ“Š è¨“ç·´è³‡è¨Š**")
            convergence = results['convergence_info']
            # å°‡æ‰€æœ‰æ•¸å€¼è½‰æ›ç‚ºå­—ä¸²ä»¥é¿å…æ··åˆé¡å‹éŒ¯èª¤
            training_info = pd.DataFrame({
                'æŒ‡æ¨™': ['å¯¦éš›è¿­ä»£æ¬¡æ•¸', 'æœ€å¤§è¿­ä»£æ¬¡æ•¸', 'æ”¶æ–‚ç‹€æ…‹', 'æ¬Šé‡åƒæ•¸ç¸½æ•¸', 'åç½®åƒæ•¸ç¸½æ•¸', 'ç¸½åƒæ•¸é‡'],
                'æ•¸å€¼': [
                    str(convergence['actual_iterations']),
                    str(convergence['max_iterations']),
                    "âœ… å·²æ”¶æ–‚" if convergence['converged'] else "âŒ æœªæ”¶æ–‚",
                    str(sum(coef.size for coef in results['mlp'].coefs_)),
                    str(sum(intercept.size for intercept in results['mlp'].intercepts_)),
                    str(sum(coef.size for coef in results['mlp'].coefs_) + 
                        sum(intercept.size for intercept in results['mlp'].intercepts_))
                ]
            })
            st.dataframe(training_info, use_container_width=True)
    
    elif model_exists:
        st.info("ğŸ“ ç™¼ç¾å·²ä¿å­˜çš„æ¨¡å‹ï¼Œä½†æœ¬æ¬¡æœªé€²è¡Œè¨“ç·´ã€‚å¦‚éœ€æŸ¥çœ‹è©³ç´°çµæœï¼Œè«‹é‡æ–°è¨“ç·´æ¨¡å‹ã€‚")
    else:
        st.warning("âš ï¸ è«‹å…ˆåœ¨ã€ŒğŸ¯ æ¨¡å‹è¨“ç·´ã€æ¨™ç±¤é è¨“ç·´æ¨¡å‹")
# --- Tab 3: å³æ™‚é æ¸¬ ---
with tab3:
    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
    if model_exists:
        try:
            loaded_mlp_model = joblib.load(MODEL_PATH)
            loaded_data_scaler = joblib.load(SCALER_PATH)
            original_X_df = st.session_state.original_X_df
            original_y = st.session_state.original_y
            # ç¢ºä¿ target_names æ˜¯å¾ session_state æˆ–å…¨åŸŸè®Šæ•¸æ­£ç¢ºç²å–
            st.success("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼Œå¯ä»¥é€²è¡Œé æ¸¬ï¼")
            st.subheader("ğŸ”® è¼¸å…¥ç‰¹å¾µå€¼é€²è¡Œé æ¸¬ æˆ– å¾åŸå§‹æ¨£æœ¬è¼‰å…¥")
            st.markdown("---")
            # ç‰¹å¾µè¼¸å…¥å€
            st.write("**å¾åŸå§‹é³¶å°¾èŠ±è³‡æ–™é›†è¼‰å…¥æ¨£æœ¬é€²è¡Œé æ¸¬ï¼š**")
            sample_id_to_load = st.selectbox(
                f"é¸æ“‡ä¸€å€‹åŸå§‹æ¨£æœ¬ ID (0 åˆ° {len(original_X_df) - 1}):",
                options=list(range(len(original_X_df))),
                index=0, # é è¨­è¼‰å…¥ç¬¬ä¸€å€‹æ¨£æœ¬
                key="sample_id_selector_tab3"
            )

            loaded_sample_features = original_X_df.iloc[sample_id_to_load] # DataFrame Series
            true_label_for_loaded_sample_index = original_y[sample_id_to_load] # int
            true_label_name_for_loaded_sample = target_names[true_label_for_loaded_sample_index] # str

            st.info(f"ç›®å‰è¼‰å…¥çš„æ¨£æœ¬ ID: {sample_id_to_load}ã€‚å…¶å·²çŸ¥çš„æ­£ç¢ºç¨®é¡æ˜¯: **{true_label_name_for_loaded_sample}**")
            st.caption("ä¸‹æ–¹çš„ç‰¹å¾µå€¼å·²å¾æ‰€é¸æ¨£æœ¬è‡ªå‹•å¡«å…¥ï¼Œæ‚¨å¯ä»¥ç›´æ¥é æ¸¬ï¼Œæˆ–ä¿®æ”¹å¾Œå†é æ¸¬ã€‚")
            # --- ã€æ–°å¢çµæŸã€‘---

            # å‰µå»ºè¼¸å…¥è¡¨å–®
            with st.form("prediction_form_tab3_v2"): # æ›´æ–° form çš„ key ä»¥ç¤ºå€åˆ¥
                st.write("**è«‹è¼¸å…¥æˆ–ç¢ºèªç‰¹å¾µå€¼ï¼š**")
                input_data = {}
                for i, feature in enumerate(selected_features): # selected_features ä¾†è‡ªå´é‚Šæ¬„
                    # ç‰¹å¾µè¼¸å…¥æ¡†çš„é è¨­å€¼ä½¿ç”¨è¼‰å…¥æ¨£æœ¬çš„ç‰¹å¾µå€¼
                    default_value_for_input = loaded_sample_features.get(feature, 0.0) # ä½¿ç”¨ .get ä»¥é˜² feature åç¨±æ„å¤–ä¸åŒ¹é…

                    # help æ–‡å­—ä¸­çš„åƒè€ƒç¯„åœï¼Œä½¿ç”¨è¨“ç·´é›†çš„çµ±è¨ˆæ•¸æ“š (åŸå§‹å°ºåº¦)
                    # å‡è¨­ X_train_full (å·²ç¸®æ”¾çš„è¨“ç·´é›†ç‰¹å¾µ) å’Œ loaded_data_scaler (åœ¨ X_train_orig ä¸Šæ“¬åˆ) å¯ç”¨
                    feature_idx_for_stats = all_feature_names.index(feature) # all_feature_names æ‡‰åŒ…å«æ‰€æœ‰åŸå§‹ç‰¹å¾µå
                    # ä»¥ä¸‹è¨ˆç®—å‡è¨­ loaded_data_scaler çš„ mean_ å’Œ scale_ ä¾†è‡ª X_train_orig
                    # ä¸¦ä¸” X_train_full[feature] æ˜¯å·²ç¸®æ”¾çš„è©²ç‰¹å¾µçš„è¨“ç·´æ•¸æ“š Series
                    original_scale_train_feature_stats = X_train_full[feature].values * loaded_data_scaler.scale_[feature_idx_for_stats] + loaded_data_scaler.mean_[feature_idx_for_stats]
                    min_val_for_help = original_scale_train_feature_stats.min()
                    max_val_for_help = original_scale_train_feature_stats.max()

                    input_data[feature] = st.number_input(
                        f'ğŸ“ {feature}',
                        value=float(default_value_for_input), # é è¨­å¡«å…¥è¼‰å…¥æ¨£æœ¬çš„ç‰¹å¾µå€¼
                        min_value=float(min_val_for_help - 1 if min_val_for_help is not np.nan else 0.0), # è™•ç†å¯èƒ½çš„nan
                        max_value=float(max_val_for_help + 1 if max_val_for_help is not np.nan else 10.0), # è™•ç†å¯èƒ½çš„nan
                        step=0.1,
                        help=f"åƒè€ƒç¯„åœ (ä¾†è‡ªè¨“ç·´é›†åˆ†å¸ƒ): {min_val_for_help:.2f} ~ {max_val_for_help:.2f}"
                    )

                predict_button = st.form_submit_button("ğŸ”® å°ä¸Šæ–¹ç‰¹å¾µå€¼é€²è¡Œé æ¸¬", type="primary", use_container_width=True)

            if predict_button:
                try:
                    # --- æº–å‚™é æ¸¬çš„è¼¸å…¥è³‡æ–™ (èˆ‡æ‚¨ç¾æœ‰é‚è¼¯é¡ä¼¼) ---
                    full_input_df = pd.DataFrame(columns=all_feature_names)
                    for feature_iter_name in all_feature_names:
                        if feature_iter_name in selected_features: # åªä½¿ç”¨å´é‚Šæ¬„é¸æ“‡çš„ç‰¹å¾µé€²è¡Œé æ¸¬
                            full_input_df.loc[0, feature_iter_name] = input_data[feature_iter_name]
                        else:
                            # å°æ–¼æ¨¡å‹è¨“ç·´æ™‚åŒ…å«ä½†æœ¬æ¬¡é æ¸¬æœªåœ¨ selected_features ä¸­çš„ç‰¹å¾µï¼Œ
                            # ä»éœ€ç”¨åŸå§‹è¨“ç·´é›†çš„å¹³å‡å€¼å¡«å……ä»¥ç¬¦åˆ scaler çš„æœŸæœ›
                            feature_idx = all_feature_names.index(feature_iter_name)
                            original_mean = loaded_data_scaler.mean_[feature_idx] # ä¾†è‡ª X_train_orig çš„å¹³å‡å€¼
                            full_input_df.loc[0, feature_iter_name] = original_mean
                    
                    input_scaled = loaded_data_scaler.transform(full_input_df) # ç¸®æ”¾æ‰€æœ‰ç‰¹å¾µ
                    # æ ¹æ“šå´é‚Šæ¬„é¸æ“‡çš„ç‰¹å¾µä¾†é¸å–å¯¦éš›ç”¨æ–¼æ¨¡å‹é æ¸¬çš„åˆ—
                    selected_indices_for_model = [all_feature_names.index(f) for f in selected_features]
                    input_for_prediction = input_scaled[:, selected_indices_for_model]
                    # --- é æ¸¬è³‡æ–™æº–å‚™çµæŸ ---

                    prediction_proba = loaded_mlp_model.predict_proba(input_for_prediction)
                    prediction_class_index = np.argmax(prediction_proba)
                    predicted_label_name = target_names[prediction_class_index]

                    # --- é¡¯ç¤ºæ¨™æº–é æ¸¬çµæœ (èˆ‡æ‚¨ç¾æœ‰é‚è¼¯é¡ä¼¼) ---
                    st.subheader("ğŸ‰ é æ¸¬çµæœ")
                    st.metric(
                        "ğŸŒ¸ æ¨¡å‹é æ¸¬é¡åˆ¥",
                        predicted_label_name,
                        delta=f"ä¿¡å¿ƒåº¦: {prediction_proba[0][prediction_class_index]:.1%}"
                    )
                    # ... (æ‚¨åŸæœ¬é¡¯ç¤ºé æ¸¬æ©Ÿç‡é•·æ¢åœ–å’Œè©³ç´°æ©Ÿç‡åˆ†å¸ƒçš„ç¨‹å¼ç¢¼ç…§èˆŠ) ...
                    # (æ©Ÿç‡é•·æ¢åœ–)
                    proba_df_display = pd.DataFrame({ # é¿å…è®Šæ•¸åè¡çª
                        'ç¨®é¡': target_names, # ä½¿ç”¨æ­£ç¢ºçš„ target_names
                        'æ©Ÿç‡': prediction_proba[0]
                    }).sort_values('æ©Ÿç‡', ascending=True)
                    fig_pred_display, ax_pred_display = plt.subplots(figsize=(8, 4)) # é¿å…è®Šæ•¸åè¡çª
                    bars_display = ax_pred_display.barh(proba_df_display['ç¨®é¡'], proba_df_display['æ©Ÿç‡'],
                                          color=['#ff7f7f' if name == predicted_label_name else '#87ceeb'
                                                 for name in proba_df_display['ç¨®é¡']])
                    ax_pred_display.set_xlabel('é æ¸¬æ©Ÿç‡')
                    ax_pred_display.set_title('å„é¡åˆ¥é æ¸¬æ©Ÿç‡åˆ†å¸ƒ')
                    ax_pred_display.set_xlim(0, 1)
                    for bar_item_display in bars_display: # é¿å…è®Šæ•¸åè¡çª
                        width_display = bar_item_display.get_width()
                        ax_pred_display.text(width_display + 0.01, bar_item_display.get_y() + bar_item_display.get_height()/2,
                                   f'{width_display:.1%}', ha='left', va='center', fontweight='bold')
                    plt.tight_layout()
                    st.pyplot(fig_pred_display)
                    plt.close(fig_pred_display)

                    # (è©³ç´°æ©Ÿç‡è¡¨)
                    st.subheader("ğŸ”¬ è©³ç´°æ©Ÿç‡åˆ†å¸ƒ")
                    detailed_proba_df_display = pd.DataFrame({
                        'èŠ±çš„ç¨®é¡': target_names, # ä½¿ç”¨æ­£ç¢ºçš„ target_names
                        'é æ¸¬æ©Ÿç‡': [f"{p:.1%}" for p in prediction_proba[0]],
                        'ä¿¡å¿ƒç­‰ç´š': ['ğŸ”¥ é«˜ä¿¡å¿ƒ' if p > 0.7 else 'âš¡ ä¸­ä¿¡å¿ƒ' if p > 0.4 else 'ğŸ’¤ ä½ä¿¡å¿ƒ'
                                   for p in prediction_proba[0]]
                    })
                    st.dataframe(detailed_proba_df_display, use_container_width=True)
                    # --- æ¨™æº–é æ¸¬çµæœé¡¯ç¤ºçµæŸ ---

                    # --- ã€ä¿®æ”¹ã€‘æ¯”å°æ¨¡å‹é æ¸¬èˆ‡ "è‡ªå‹•è¼‰å…¥çš„" åŸå§‹æ¨£æœ¬æ­£ç¢ºç­”æ¡ˆ ---
                    st.subheader("ğŸ” é æ¸¬èˆ‡åŸå§‹æ¨£æœ¬ç­”æ¡ˆæ¯”å°")
                    if predicted_label_name == true_label_name_for_loaded_sample: # true_label_name_for_loaded_sample ä¾†è‡ªé¸æ“‡çš„æ¨£æœ¬
                        st.success(f"âœ… **ä¸€è‡´ï¼** æ¨¡å‹é æ¸¬ ({predicted_label_name}) èˆ‡æ‰€é¸åŸå§‹æ¨£æœ¬ (ID: {sample_id_to_load}) çš„æ­£ç¢ºç­”æ¡ˆ ({true_label_name_for_loaded_sample}) ç›¸åŒã€‚")
                    else:
                        st.error(f"âŒ **ä¸ä¸€è‡´ï¼** æ¨¡å‹é æ¸¬ç‚º ({predicted_label_name})ï¼Œä½†æ‰€é¸åŸå§‹æ¨£æœ¬ (ID: {sample_id_to_load}) çš„æ­£ç¢ºç­”æ¡ˆæ˜¯ ({true_label_name_for_loaded_sample})ã€‚")
                    # --- ã€ä¿®æ”¹çµæŸã€‘---

                except Exception as e:
                    st.error(f"âŒ é æ¸¬éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    import traceback
                    st.text("è©³ç´°éŒ¯èª¤å †ç–Šï¼š")
                    st.code(traceback.format_exc())
        # ... (è™•ç†æ¨¡å‹è¼‰å…¥å¤±æ•—å’Œæ¨¡å‹æœªè¨“ç·´çš„ except å’Œ else å€å¡Šç…§èˆŠ) ...
        except Exception as e:
            st.error(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼š{e}")
            import traceback
            st.text("è©³ç´°éŒ¯èª¤å †ç–Šï¼š")
            st.code(traceback.format_exc())
    else: # model_exists is False
        st.warning("âš ï¸ è«‹å…ˆåœ¨ã€ŒğŸ¯ æ¨¡å‹è¨“ç·´ã€æ¨™ç±¤é è¨“ç·´æ¨¡å‹")
        st.info("ğŸ’¡ è¨“ç·´å®Œæˆå¾Œå³å¯åœ¨æ­¤é€²è¡Œå³æ™‚é æ¸¬")
            
            

# --- é è…³ ---
st.markdown("---")
st.subheader("ğŸ“š ç›¸é—œè³‡æº")

st.markdown("**ğŸ“Š è³‡æ–™ä¾†æºï¼š** [Scikit-learn Iris Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)")
st.markdown("**âš¡ éƒ¨ç½²å¹³å°ï¼š** [Streamlit Cloud](https://streamlit.io/cloud)")
st.markdown("**ğŸ› ï¸ æŠ€è¡“æ¡†æ¶ï¼š** Streamlit + Scikit-learn + Matplotlib")
st.markdown("**âš›ï¸ æ¨¡å‹èªªæ˜ï¼š** æœ¬æ‡‰ç”¨ä½¿ç”¨å¤šå±¤æ„ŸçŸ¥å™¨ (MLP) é€²è¡Œé³¶å°¾èŠ±åˆ†é¡ï¼Œæ”¯æ´å®Œæ•´çš„è¶…åƒæ•¸èª¿æ•´èˆ‡çµæœåˆ†æ")